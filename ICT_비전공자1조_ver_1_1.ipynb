{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aebonlee/chatbot_01team/blob/main/ICT_%EB%B9%84%EC%A0%84%EA%B3%B5%EC%9E%901%EC%A1%B0_ver_1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ac32751",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ac32751",
        "outputId": "fbd88807-cbbd-4b1d-db21-58668fe00931",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.31-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.74)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.14)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.100.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Downloading langchain_openai-0.3.31-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-openai\n",
            "Successfully installed langchain-openai-0.3.31\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-openai tiktoken python-dotenv\n",
        "#!pip install orjson\n",
        "#!pip install gradio==4.*\n",
        "#!pip install trafilatura"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OlbCnIMa9df8"
      },
      "id": "OlbCnIMa9df8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "471e96aa",
      "metadata": {
        "id": "471e96aa"
      },
      "outputs": [],
      "source": [
        "import os, json\n",
        "from typing import List, Dict, Any\n",
        "import gradio as gr\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import SystemMessage, HumanMessage"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ë©‹ì§„ UI ì „"
      ],
      "metadata": {
        "id": "mUj-dUabywK_"
      },
      "id": "mUj-dUabywK_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7eb0eea4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "7eb0eea4",
        "outputId": "02e2b9d4-21fa-47eb-c501-7fb1fd6efaf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3431024483.py:195: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot=gr.Chatbot(height=520, show_copy_button=True),\n",
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:328: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://9aa7893cb3c115a885.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9aa7893cb3c115a885.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# APIí‚¤ ë° ëª¨ë¸ ì„¤ì •\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "OPENAI_MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
        "llm_g = ChatOpenAI(model=MODEL, temperature=0)     # ê°€ë“œ/íŒì •/ë¦¬ë¼ì´íŠ¸\n",
        "llm_x = ChatOpenAI(model=MODEL, temperature=0.2)   # ìì²´ ì¶”ì¶œ\n",
        "llm_s = ChatOpenAI(model=MODEL, temperature=0.2)   # ìš”ì•½\n",
        "\n",
        "KNOWLEDGE_CUTOFF = \"2024-06\"\n",
        "\n",
        "STYLE_SYS = (\n",
        "    \"í•œêµ­ì–´ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë§íˆ¬ëŠ” ì¹œì ˆí•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ìœ ì§€í•©ë‹ˆë‹¤. \"\n",
        "    \"í•µì‹¬ì€ ê°„ê²°í•˜ê²Œ ì „ë‹¬í•˜ë˜, ì „ë ¥Â·ì—ë„ˆì§€Â·ëª¨ë¹Œë¦¬í‹° ë¶„ì•¼ì˜ ìˆ˜ì¹˜/ë‹¨ìœ„/ê¸°í˜¸(Î·, THD, pf, pu, kW, kWh, Â°C ë“±)ëŠ” ë³´ì¡´í•©ë‹ˆë‹¤. \"\n",
        "    \"ë¶ˆí™•ì‹¤í•˜ê±°ë‚˜ ê¸°ì–µì´ ëª¨í˜¸í•œ ë‚´ìš©ì€ 'ë¶ˆí™•ì‹¤'ë¡œ í‘œì‹œí•˜ê³  ì¶”ì •Â·ì¼ë°˜ë¡ ì€ ëª…í™•íˆ êµ¬ë¶„í•©ë‹ˆë‹¤. ê³¼ì¥ í‘œí˜„ì€ ì§€ì–‘í•©ë‹ˆë‹¤.\"\n",
        ")\n",
        "\n",
        "ALLOW = [\n",
        "    \"ì „ë ¥\",\"ì—ë„ˆì§€\",\"ëª¨ë¹Œë¦¬í‹°\",\"EV\",\"ì¶©ì „\",\"ë°°í„°ë¦¬\",\"ì‹ ì¬ìƒ\",\"íƒœì–‘ê´‘\",\"í’ë ¥\",\"ìˆ˜ì†Œ\",\n",
        "    \"ì¸ë²„í„°\",\"ì»¨ë²„í„°\",\"PCS\",\"BESS\",\"V2G\",\"EMS\",\"DER\",\"DR\",\"ìŠ¤ë§ˆíŠ¸ê·¸ë¦¬ë“œ\",\"ì†¡ë°°ì „\",\"ë³€ì „\",\n",
        "    \"THD\",\"pf\",\"pu\",\"ê³ ì¡°íŒŒ\",\"ì „ì••ì•ˆì •ë„\",\"ì‹ ë¢°ë„\",\"ì‹œì¥\",\"íŠ¸ë Œë“œ\",\"ë™í–¥\"\n",
        "]\n",
        "\n",
        "def jload(s: str, default: Any) -> Any:\n",
        "    try:\n",
        "        return json.loads(s)\n",
        "    except:\n",
        "        return default\n",
        "\n",
        "def evaluate_answerability(q: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    JSON ì˜ˆì‹œ\n",
        "    {\n",
        "      \"answerable\": true|false,\n",
        "      \"reasons\": [\"off_topic\",\"requires_web\",\"knowledge_cutoff\",\"ambiguous\",\"too_broad\",\"privacy_sensitive\",\"unsafe\"],\n",
        "      \"explain\": \"ì™œ ë°”ë¡œ ë‹µí•˜ê¸° ì–´ë ¤ìš´ì§€ í•œ ì¤„ ì„¤ëª…(ì¡´ëŒ“ë§)\",\n",
        "      \"ask_for\": [\"ëˆ„ë½ëœ êµ¬ì²´ ì •ë³´ í•­ëª©ë“¤\"],\n",
        "      \"rewrite_examples\": [\"ê¶Œì¥ ì¬ì§ˆë¬¸ 1\",\"ê¶Œì¥ ì¬ì§ˆë¬¸ 2\"]\n",
        "    }\n",
        "    \"\"\"\n",
        "    policy = (\n",
        "        \"ë‹¹ì‹ ì€ ì§ˆì˜ê°€ ì•„ë˜ ì œì•½ì—ì„œ ë‹µë³€ ê°€ëŠ¥í•œì§€ íŒì •í•©ë‹ˆë‹¤.\\n\"\n",
        "        f\"- ì§€ì‹ ì»·ì˜¤í”„: {KNOWLEDGE_CUTOFF}\\n\"\n",
        "        \"- ì™¸ë¶€ ì›¹ ê²€ìƒ‰/ì‹¤ì‹œê°„ ë°ì´í„° ì‚¬ìš© ë¶ˆê°€\\n\"\n",
        "        \"- ë„ë©”ì¸: ì „ë ¥Â·ì—ë„ˆì§€Â·ëª¨ë¹Œë¦¬í‹° ì¤‘ì‹¬\\n\"\n",
        "        \"- ê°œì¸ì‹ë³„/ë¯¼ê°ì •ë³´, ë¶ˆë²•/ìœ„í—˜, ì˜ë£ŒÂ·ë²•ë¥ Â·íˆ¬ì ê°œë³„ ì¡°ì–¸ì€ ë¶ˆê°€\\n\"\n",
        "        \"- ëª¨í˜¸/ê³¼ë„í•˜ê²Œ ê´‘ë²”ìœ„/ì •ì˜ê°€ ë¶ˆëª…í™•í•˜ë©´ êµ¬ì²´í™”ê°€ í•„ìš”\\n\"\n",
        "        \"ìœ„ ì¡°ê±´ì„ ë°”íƒ•ìœ¼ë¡œ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.\"\n",
        "    )\n",
        "    schema = {\n",
        "        \"answerable\": True,\n",
        "        \"reasons\": [],\n",
        "        \"explain\": \"\",\n",
        "        \"ask_for\": [],\n",
        "        \"rewrite_examples\": []\n",
        "    }\n",
        "    prompt = (\n",
        "        policy + \"\\n\\n\"\n",
        "        \"ê°€ëŠ¥í•˜ë©´ ì‚¬ìš©ìê°€ ë°”ë¡œ ì“¸ ìˆ˜ ìˆëŠ” 'ì¬ì§ˆë¬¸ ì˜ˆì‹œ'ë¥¼ 1~3ê°œ ì œì•ˆí•˜ì„¸ìš” \"\n",
        "        \"(ì „ë ¥Â·ì—ë„ˆì§€ ë§¥ë½ì˜ ì§€í‘œ/ë²”ìœ„/ì§€ì—­/ê¸°ê°„/ëŒ€ìƒ ë“±ì„ í¬í•¨). \"\n",
        "        \"JSONë§Œ ì¶œë ¥. ìŠ¤í‚¤ë§ˆëŠ” ë‹¤ìŒê³¼ ê°™ìŒ:\\n\"\n",
        "        + json.dumps(schema, ensure_ascii=False) + \"\\n\\n\"\n",
        "        f\"ì§ˆë¬¸: {q}\"\n",
        "    )\n",
        "    out = llm_g.invoke([\n",
        "        SystemMessage(content=STYLE_SYS),\n",
        "        SystemMessage(content=\"JSON only.\"),\n",
        "        HumanMessage(content=prompt)\n",
        "    ]).content\n",
        "    data = jload(out, schema)\n",
        "    # íœ´ë¦¬ìŠ¤í‹± ë³´ê°•: ì‹¤ì‹œê°„/ìµœì‹ /ê°€ê²©/ì£¼ê°€/ë‚ ì”¨ ë“±\n",
        "    recent_triggers = [\"ì‹¤ì‹œê°„\",\"í˜„ì¬\",\"ì˜¤ëŠ˜\",\"ë°©ê¸ˆ\",\"ì§€ê¸ˆ\",\"ìµœì‹ \",\"ì£¼ê°€\",\"í™˜ìœ¨\",\"ë‚ ì”¨\",\"ìŠ¤ì½”ì–´\",\"ì†ë³´\",\"ë¼ì´ë¸Œ\"]\n",
        "    if any(t in q for t in recent_triggers):\n",
        "        data[\"answerable\"] = False\n",
        "        if \"requires_web\" not in data[\"reasons\"]:\n",
        "            data[\"reasons\"].append(\"requires_web\")\n",
        "        data[\"explain\"] = data.get(\"explain\") or \"ì‹¤ì‹œê°„Â·ìµœì‹  ë°ì´í„°ëŠ” ì›¹ ì ‘ê·¼ ì—†ì´ ì •í™•íˆ ì•ˆë‚´ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤.\"\n",
        "    # ë„ë©”ì¸ ì²´í¬\n",
        "    if not any(k.lower() in q.lower() for k in ALLOW):\n",
        "        if \"off_topic\" not in data[\"reasons\"]:\n",
        "            data[\"reasons\"].append(\"off_topic\")\n",
        "    return data\n",
        "\n",
        "def guidance_message(evald: Dict[str, Any]) -> str:\n",
        "    reasons_ko = {\n",
        "        \"off_topic\": \"ì¤€ë¹„ëœ ì£¼ì œ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤.\",\n",
        "        \"requires_web\": \"ì‹¤ì‹œê°„/ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œ ì£¼ì œì…ë‹ˆë‹¤.\",\n",
        "        \"knowledge_cutoff\": f\"ì§€ì‹ ì»·ì˜¤í”„({KNOWLEDGE_CUTOFF}) ì´í›„ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.\",\n",
        "        \"ambiguous\": \"ì§ˆë¬¸ì´ ëª¨í˜¸í•©ë‹ˆë‹¤.\",\n",
        "        \"too_broad\": \"ì§ˆë¬¸ ë²”ìœ„ê°€ ë„ˆë¬´ ë„“ìŠµë‹ˆë‹¤.\",\n",
        "        \"privacy_sensitive\": \"ê°œì¸ì •ë³´Â·ë¯¼ê°ì •ë³´ë¼ì„œ ë„ì™€ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤.\",\n",
        "        \"unsafe\": \"ìœ„í—˜/ë¶€ì ì ˆí•œ ìš”ì²­ì…ë‹ˆë‹¤.\"\n",
        "    }\n",
        "    lines = [\"ìš”ì²­í•˜ì‹  ë‚´ìš©ì€ ë°”ë¡œ ì •í™•íˆ ë‹µë³€ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤.\"]\n",
        "    if evald.get(\"explain\"):\n",
        "        lines.append(f\"ì‚¬ìœ : {evald['explain']}\")\n",
        "    if evald.get(\"reasons\"):\n",
        "        labs = [reasons_ko.get(r, r) for r in evald[\"reasons\"]]\n",
        "        lines.append(\"íŒì •: \" + \", \".join(labs))\n",
        "    if evald.get(\"ask_for\"):\n",
        "        lines.append(\"\\në‹¤ìŒ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ë” ì •í™•íˆ ë„ì™€ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\")\n",
        "        lines += [f\"- {a}\" for a in evald[\"ask_for\"][:6]]\n",
        "    if evald.get(\"rewrite_examples\"):\n",
        "        lines.append(\"\\nì´ë ‡ê²Œ ì§ˆë¬¸í•´ ë³´ì‹œë©´ ì¢‹ìŠµë‹ˆë‹¤:\")\n",
        "        lines += [f\"- {r}\" for r in evald[\"rewrite_examples\"][:3]]\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def make_subquestions(q: str) -> List[str]:\n",
        "    prompt = (\n",
        "        \"ë‹¤ìŒ ì§ˆë¬¸ì„ 3~5ê°œì˜ í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´í•˜ì„¸ìš”. ìµœì‹ ì„± ì´ìŠˆê°€ ìˆìœ¼ë©´ 'ìµœê·¼ ë™í–¥ í™•ì¸'ì„ í¬í•¨í•©ë‹ˆë‹¤. \"\n",
        "        \"JSONë§Œ ì¶œë ¥: {\\\"subs\\\": [\\\"...\\\"]}\\n\\n\"\n",
        "        f\"ì§ˆë¬¸: {q}\"\n",
        "    )\n",
        "    out = llm_x.invoke([SystemMessage(content=\"JSON only.\"), HumanMessage(content=prompt)]).content\n",
        "    data = jload(out, {\"subs\":[q]})\n",
        "    subs = [s for s in data.get(\"subs\", []) if isinstance(s, str) and s.strip()]\n",
        "    return subs[:5] if subs else [q]\n",
        "\n",
        "def extract_self(subq: str) -> Dict[str, Any]:\n",
        "    schema = {\n",
        "        \"bullets\": [],\n",
        "        \"metrics\": [{\"name\": \"\", \"value\": \"\", \"unit\": \"\", \"context\": \"\", \"confidence\": \"low|medium|high\"}],\n",
        "        \"claims\": [{\"statement\": \"\", \"confidence\": \"low|medium|high\"}],\n",
        "        \"uncertainties\": []\n",
        "    }\n",
        "    prompt = (\n",
        "        \"ì™¸ë¶€ ê²€ìƒ‰ ì—†ì´, ì¼ë°˜ ì§€ì‹ ë²”ìœ„ì—ì„œ ì•„ë˜ í•˜ìœ„ ì§ˆë¬¸ì˜ í•µì‹¬ ì •ë³´ë¥¼ ìì²´ì ìœ¼ë¡œ ìˆ˜ì§‘Â·ì •ë¦¬í•´ ì£¼ì„¸ìš”. \"\n",
        "        \"í™•ì‹ ë„ í‘œê¸°, ë¶ˆí™•ì‹¤/ê·¼ì‚¬ëŠ” ëª…ì‹œ. JSONë§Œ ë°˜í™˜.\\n\\n\"\n",
        "        + json.dumps(schema, ensure_ascii=False)\n",
        "        + \"\\n\\ní•˜ìœ„ ì§ˆë¬¸:\\n\" + subq\n",
        "        + f\"\\n\\nì§€ì‹ ì»·ì˜¤í”„: {KNOWLEDGE_CUTOFF}\"\n",
        "    )\n",
        "    out = llm_x.invoke([\n",
        "        SystemMessage(content=STYLE_SYS),\n",
        "        SystemMessage(content=\"ìˆ˜ì¹˜Â·ë‹¨ìœ„Â·ê¸°í˜¸ ë³´ì¡´, ë¶ˆí™•ì‹¤ì„± ëª…ì‹œ, JSON only.\"),\n",
        "        HumanMessage(content=prompt)\n",
        "    ]).content\n",
        "    return jload(out, {\"bullets\": [], \"metrics\": [], \"claims\": [], \"uncertainties\": []})\n",
        "\n",
        "def aggregate(parts: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    bullets, seen = [], set()\n",
        "    metrics, claims, uncertainties = [], [], []\n",
        "    for p in parts:\n",
        "        for b in p.get(\"bullets\", []):\n",
        "            s = b.strip()\n",
        "            if s and s not in seen:\n",
        "                seen.add(s); bullets.append(s)\n",
        "        metrics.extend([m for m in p.get(\"metrics\", []) if isinstance(m, dict)])\n",
        "        claims.extend([c for c in p.get(\"claims\", []) if isinstance(c, dict)])\n",
        "        uncertainties.extend([u for u in p.get(\"uncertainties\", []) if isinstance(u, str)])\n",
        "    return {\n",
        "        \"bullets\": bullets[:14],\n",
        "        \"metrics\": metrics[:12],\n",
        "        \"claims\": claims[:12],\n",
        "        \"uncertainties\": uncertainties[:8]\n",
        "    }\n",
        "\n",
        "def summarize(query: str, agg: Dict[str, Any]) -> str:\n",
        "    prompt = (\n",
        "        \"ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¹´ë“œí˜• ìš”ì•½ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.\\n\"\n",
        "        \"- ì„¹ì…˜ 3~6ê°œ, ê° 3~5ì¤„, ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ì¡´ëŒ“ë§\\n\"\n",
        "        \"- ë‹¨ìœ„/ìˆ˜ì¹˜/ê¸°í˜¸ ë³´ì¡´, ì¶”ì •/ë¶ˆí™•ì‹¤ ëª…ì‹œ\\n\"\n",
        "        f\"- ì§€ì‹ ì»·ì˜¤í”„: {KNOWLEDGE_CUTOFF}\\n\\n\"\n",
        "        f\"ì§ˆë¬¸: {query}\\n\"\n",
        "        f\"í•µì‹¬í¬ì¸íŠ¸: {json.dumps(agg.get('bullets', []), ensure_ascii=False)}\\n\"\n",
        "        f\"ì§€í‘œ: {json.dumps(agg.get('metrics', []), ensure_ascii=False)}\\n\"\n",
        "        f\"ì£¼ì¥: {json.dumps(agg.get('claims', []), ensure_ascii=False)}\\n\"\n",
        "        f\"ë¶ˆí™•ì‹¤: {json.dumps(agg.get('uncertainties', []), ensure_ascii=False)}\\n\"\n",
        "    )\n",
        "    return llm_s.invoke([\n",
        "        SystemMessage(content=STYLE_SYS),\n",
        "        SystemMessage(content=\"ê°„ê²°Â·ì •í™•Â·ê·¼ê±° ìˆ˜ì¤€ í‘œì‹œ\"),\n",
        "        HumanMessage(content=prompt)\n",
        "    ]).content\n",
        "\n",
        "def run(query: str) -> str:\n",
        "    if not query.strip():\n",
        "        return \"ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.\"\n",
        "    # â–¶ NEW: ë¨¼ì € ë‹µë³€ ê°€ëŠ¥ì„± í‰ê°€\n",
        "    ev = evaluate_answerability(query)\n",
        "    if not ev.get(\"answerable\", True):\n",
        "        return guidance_message(ev)\n",
        "    # â–¶ ê°€ëŠ¥í•˜ë©´ ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ ìˆ˜í–‰\n",
        "    subs = make_subquestions(ev.get(\"rewrite_examples\",[query])[0] if ev.get(\"rewrite_examples\") else query)\n",
        "    parts = [extract_self(s) for s in subs]\n",
        "    agg = aggregate(parts)\n",
        "    return summarize(query, agg)\n",
        "\n",
        "def _chat_handler(user_text: str, history: list[list[str]]) -> str:\n",
        "    # ë©€í‹°í„´ ë¬¸ë§¥ì„ ì“°ì§€ ì•Šê³ , ê¸°ì¡´ ë‹¨ì¼í„´ íŒŒì´í”„ë¼ì¸(run) ê·¸ëŒ€ë¡œ í˜¸ì¶œ\n",
        "    return run(user_text)\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    fn=_chat_handler,\n",
        "    chatbot=gr.Chatbot(height=520, show_copy_button=True),\n",
        "    title=\"ì „ë ¥Â·ì—ë„ˆì§€ ìš”ì•½ ì±—ë´‡\",\n",
        "    description=\"ì‹œì¥/ì‚°ì—…/ì—…ê³„ ë™í–¥ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µí•©ë‹ˆë‹¤.\",\n",
        "    examples=[\n",
        "        \"í•œêµ­ ì¬ìƒì—ë„ˆì§€ ë³´ê¸‰ ë™í–¥ ìš”ì•½\",\n",
        "        \"BESS ì‹œì¥ íŠ¸ë Œë“œ í•µì‹¬ í¬ì¸íŠ¸\",\n",
        "        \"EV ì¶©ì „ ì¸í”„ë¼ ìµœê·¼ ì´ìŠˆ ì •ë¦¬\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "demo.launch(share=True)  # Colabì€ share=True í•„ìš”"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë” ì´ìœ UI(ìµœì¢…)"
      ],
      "metadata": {
        "id": "1F2CM1_yys4a"
      },
      "id": "1F2CM1_yys4a"
    },
    {
      "cell_type": "code",
      "source": [
        "# APIí‚¤ ë° ëª¨ë¸ ì„¤ì •\n",
        "import os\n",
        "import json\n",
        "from typing import Any, Dict, List\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "import gradio as gr\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "OPENAI_MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
        "llm_g = ChatOpenAI(model=MODEL, temperature=0)     # ê°€ë“œ/íŒì •/ë¦¬ë¼ì´íŠ¸\n",
        "llm_x = ChatOpenAI(model=MODEL, temperature=0.2)   # ìì²´ ì¶”ì¶œ\n",
        "llm_s = ChatOpenAI(model=MODEL, temperature=0.2)   # ìš”ì•½\n",
        "\n",
        "KNOWLEDGE_CUTOFF = \"2024-06\"\n",
        "\n",
        "STYLE_SYS = (\n",
        "    \"í•œêµ­ì–´ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë§íˆ¬ëŠ” ì¹œì ˆí•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ìœ ì§€í•©ë‹ˆë‹¤. \"\n",
        "    \"í•µì‹¬ì€ ê°„ê²°í•˜ê²Œ ì „ë‹¬í•˜ë˜, ì „ë ¥Â·ì—ë„ˆì§€Â·ëª¨ë¹Œë¦¬í‹° ë¶„ì•¼ì˜ ìˆ˜ì¹˜/ë‹¨ìœ„/ê¸°í˜¸(Î·, THD, pf, pu, kW, kWh, Â°C ë“±)ëŠ” ë³´ì¡´í•©ë‹ˆë‹¤. \"\n",
        "    \"ë¶ˆí™•ì‹¤í•˜ê±°ë‚˜ ê¸°ì–µì´ ëª¨í˜¸í•œ ë‚´ìš©ì€ 'ë¶ˆí™•ì‹¤'ë¡œ í‘œì‹œí•˜ê³  ì¶”ì •Â·ì¼ë°˜ë¡ ì€ ëª…í™•íˆ êµ¬ë¶„í•©ë‹ˆë‹¤. ê³¼ì¥ í‘œí˜„ì€ ì§€ì–‘í•©ë‹ˆë‹¤.\"\n",
        ")\n",
        "\n",
        "ALLOW = [\n",
        "    \"ì „ë ¥\",\"ì—ë„ˆì§€\",\"ëª¨ë¹Œë¦¬í‹°\",\"EV\",\"ì¶©ì „\",\"ë°°í„°ë¦¬\",\"ì‹ ì¬ìƒ\",\"íƒœì–‘ê´‘\",\"í’ë ¥\",\"ìˆ˜ì†Œ\",\n",
        "    \"ì¸ë²„í„°\",\"ì»¨ë²„í„°\",\"PCS\",\"BESS\",\"V2G\",\"EMS\",\"DER\",\"DR\",\"ìŠ¤ë§ˆíŠ¸ê·¸ë¦¬ë“œ\",\"ì†¡ë°°ì „\",\"ë³€ì „\",\n",
        "    \"THD\",\"pf\",\"pu\",\"ê³ ì¡°íŒŒ\",\"ì „ì••ì•ˆì •ë„\",\"ì‹ ë¢°ë„\",\"ì‹œì¥\",\"íŠ¸ë Œë“œ\",\"ë™í–¥\"\n",
        "]\n",
        "\n",
        "def jload(s: str, default: Any) -> Any:\n",
        "    try:\n",
        "        return json.loads(s)\n",
        "    except:\n",
        "        return default\n",
        "\n",
        "def evaluate_answerability(q: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    JSON ì˜ˆì‹œ\n",
        "    {\n",
        "      \"answerable\": true|false,\n",
        "      \"reasons\": [\"off_topic\",\"requires_web\",\"knowledge_cutoff\",\"ambiguous\",\"too_broad\",\"privacy_sensitive\",\"unsafe\"],\n",
        "      \"explain\": \"ì™œ ë°”ë¡œ ë‹µí•˜ê¸° ì–´ë ¤ìš´ì§€ í•œ ì¤„ ì„¤ëª…(ì¡´ëŒ“ë§)\",\n",
        "      \"ask_for\": [\"ëˆ„ë½ëœ êµ¬ì²´ ì •ë³´ í•­ëª©ë“¤\"],\n",
        "      \"rewrite_examples\": [\"ê¶Œì¥ ì¬ì§ˆë¬¸ 1\",\"ê¶Œì¥ ì¬ì§ˆë¬¸ 2\"]\n",
        "    }\n",
        "    \"\"\"\n",
        "    policy = (\n",
        "        \"ë‹¹ì‹ ì€ ì§ˆì˜ê°€ ì•„ë˜ ì œì•½ì—ì„œ ë‹µë³€ ê°€ëŠ¥í•œì§€ íŒì •í•©ë‹ˆë‹¤.\\n\"\n",
        "        f\"- ì§€ì‹ ì»·ì˜¤í”„: {KNOWLEDGE_CUTOFF}\\n\"\n",
        "        \"- ì™¸ë¶€ ì›¹ ê²€ìƒ‰/ì‹¤ì‹œê°„ ë°ì´í„° ì‚¬ìš© ë¶ˆê°€\\n\"\n",
        "        \"- ë„ë©”ì¸: ì „ë ¥Â·ì—ë„ˆì§€Â·ëª¨ë¹Œë¦¬í‹° ì¤‘ì‹¬\\n\"\n",
        "        \"- ê°œì¸ì‹ë³„/ë¯¼ê°ì •ë³´, ë¶ˆë²•/ìœ„í—˜, ì˜ë£ŒÂ·ë²•ë¥ Â·íˆ¬ì ê°œë³„ ì¡°ì–¸ì€ ë¶ˆê°€\\n\"\n",
        "        \"- ëª¨í˜¸/ê³¼ë„í•˜ê²Œ ê´‘ë²”ìœ„/ì •ì˜ê°€ ë¶ˆëª…í™•í•˜ë©´ êµ¬ì²´í™”ê°€ í•„ìš”\\n\"\n",
        "        \"ìœ„ ì¡°ê±´ì„ ë°”íƒ•ìœ¼ë¡œ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.\"\n",
        "    )\n",
        "    schema = {\n",
        "        \"answerable\": True,\n",
        "        \"reasons\": [],\n",
        "        \"explain\": \"\",\n",
        "        \"ask_for\": [],\n",
        "        \"rewrite_examples\": []\n",
        "    }\n",
        "    prompt = (\n",
        "        policy + \"\\n\\n\"\n",
        "        \"ê°€ëŠ¥í•˜ë©´ ì‚¬ìš©ìê°€ ë°”ë¡œ ì“¸ ìˆ˜ ìˆëŠ” 'ì¬ì§ˆë¬¸ ì˜ˆì‹œ'ë¥¼ 1~3ê°œ ì œì•ˆí•˜ì„¸ìš” \"\n",
        "        \"(ì „ë ¥Â·ì—ë„ˆì§€ ë§¥ë½ì˜ ì§€í‘œ/ë²”ìœ„/ì§€ì—­/ê¸°ê°„/ëŒ€ìƒ ë“±ì„ í¬í•¨). \"\n",
        "        \"JSONë§Œ ì¶œë ¥. ìŠ¤í‚¤ë§ˆëŠ” ë‹¤ìŒê³¼ ê°™ìŒ:\\n\"\n",
        "        + json.dumps(schema, ensure_ascii=False) + \"\\n\\n\"\n",
        "        f\"ì§ˆë¬¸: {q}\"\n",
        "    )\n",
        "    out = llm_g.invoke([\n",
        "        SystemMessage(content=STYLE_SYS),\n",
        "        SystemMessage(content=\"JSON only.\"),\n",
        "        HumanMessage(content=prompt)\n",
        "    ]).content\n",
        "    data = jload(out, schema)\n",
        "    # íœ´ë¦¬ìŠ¤í‹± ë³´ê°•: ì‹¤ì‹œê°„/ìµœì‹ /ê°€ê²©/ì£¼ê°€/ë‚ ì”¨ ë“±\n",
        "    recent_triggers = [\"ì‹¤ì‹œê°„\",\"í˜„ì¬\",\"ì˜¤ëŠ˜\",\"ë°©ê¸ˆ\",\"ì§€ê¸ˆ\",\"ìµœì‹ \",\"ì£¼ê°€\",\"í™˜ìœ¨\",\"ë‚ ì”¨\",\"ìŠ¤ì½”ì–´\",\"ì†ë³´\",\"ë¼ì´ë¸Œ\"]\n",
        "    if any(t in q for t in recent_triggers):\n",
        "        data[\"answerable\"] = False\n",
        "        if \"requires_web\" not in data[\"reasons\"]:\n",
        "            data[\"reasons\"].append(\"requires_web\")\n",
        "        data[\"explain\"] = data.get(\"explain\") or \"ì‹¤ì‹œê°„Â·ìµœì‹  ë°ì´í„°ëŠ” ì›¹ ì ‘ê·¼ ì—†ì´ ì •í™•íˆ ì•ˆë‚´ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤.\"\n",
        "    # ë„ë©”ì¸ ì²´í¬\n",
        "    if not any(k.lower() in q.lower() for k in ALLOW):\n",
        "        if \"off_topic\" not in data[\"reasons\"]:\n",
        "            data[\"reasons\"].append(\"off_topic\")\n",
        "    return data\n",
        "\n",
        "def guidance_message(evald: Dict[str, Any]) -> str:\n",
        "    reasons_ko = {\n",
        "        \"off_topic\": \"ì¤€ë¹„ëœ ì£¼ì œ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤.\",\n",
        "        \"requires_web\": \"ì‹¤ì‹œê°„/ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œ ì£¼ì œì…ë‹ˆë‹¤.\",\n",
        "        \"knowledge_cutoff\": f\"ì§€ì‹ ì»·ì˜¤í”„({KNOWLEDGE_CUTOFF}) ì´í›„ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.\",\n",
        "        \"ambiguous\": \"ì§ˆë¬¸ì´ ëª¨í˜¸í•©ë‹ˆë‹¤.\",\n",
        "        \"too_broad\": \"ì§ˆë¬¸ ë²”ìœ„ê°€ ë„ˆë¬´ ë„“ìŠµë‹ˆë‹¤.\",\n",
        "        \"privacy_sensitive\": \"ê°œì¸ì •ë³´Â·ë¯¼ê°ì •ë³´ë¼ì„œ ë„ì™€ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤.\",\n",
        "        \"unsafe\": \"ìœ„í—˜/ë¶€ì ì ˆí•œ ìš”ì²­ì…ë‹ˆë‹¤.\"\n",
        "    }\n",
        "    lines = [\"ìš”ì²­í•˜ì‹  ë‚´ìš©ì€ ë°”ë¡œ ì •í™•íˆ ë‹µë³€ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤.\"]\n",
        "    if evald.get(\"explain\"):\n",
        "        lines.append(f\"ì‚¬ìœ : {evald['explain']}\")\n",
        "    if evald.get(\"reasons\"):\n",
        "        labs = [reasons_ko.get(r, r) for r in evald[\"reasons\"]]\n",
        "        lines.append(\"íŒì •: \" + \", \".join(labs))\n",
        "    if evald.get(\"ask_for\"):\n",
        "        lines.append(\"\\në‹¤ìŒ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ë” ì •í™•íˆ ë„ì™€ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\")\n",
        "        lines += [f\"- {a}\" for a in evald[\"ask_for\"][:6]]\n",
        "    if evald.get(\"rewrite_examples\"):\n",
        "        lines.append(\"\\nì´ë ‡ê²Œ ì§ˆë¬¸í•´ ë³´ì‹œë©´ ì¢‹ìŠµë‹ˆë‹¤:\")\n",
        "        lines += [f\"- {r}\" for r in evald[\"rewrite_examples\"][:3]]\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def make_subquestions(q: str) -> List[str]:\n",
        "    prompt = (\n",
        "        \"ë‹¤ìŒ ì§ˆë¬¸ì„ 3~5ê°œì˜ í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´í•˜ì„¸ìš”. ìµœì‹ ì„± ì´ìŠˆê°€ ìˆìœ¼ë©´ 'ìµœê·¼ ë™í–¥ í™•ì¸'ì„ í¬í•¨í•©ë‹ˆë‹¤. \"\n",
        "        \"JSONë§Œ ì¶œë ¥: {\\\"subs\\\": [\\\"...\\\"]}\\n\\n\"\n",
        "        f\"ì§ˆë¬¸: {q}\"\n",
        "    )\n",
        "    out = llm_x.invoke([SystemMessage(content=\"JSON only.\"), HumanMessage(content=prompt)]).content\n",
        "    data = jload(out, {\"subs\":[q]})\n",
        "    subs = [s for s in data.get(\"subs\", []) if isinstance(s, str) and s.strip()]\n",
        "    return subs[:5] if subs else [q]\n",
        "\n",
        "def extract_self(subq: str) -> Dict[str, Any]:\n",
        "    schema = {\n",
        "        \"bullets\": [],\n",
        "        \"metrics\": [{\"name\": \"\", \"value\": \"\", \"unit\": \"\", \"context\": \"\", \"confidence\": \"low|medium|high\"}],\n",
        "        \"claims\": [{\"statement\": \"\", \"confidence\": \"low|medium|high\"}],\n",
        "        \"uncertainties\": []\n",
        "    }\n",
        "    prompt = (\n",
        "        \"ì™¸ë¶€ ê²€ìƒ‰ ì—†ì´, ì¼ë°˜ ì§€ì‹ ë²”ìœ„ì—ì„œ ì•„ë˜ í•˜ìœ„ ì§ˆë¬¸ì˜ í•µì‹¬ ì •ë³´ë¥¼ ìì²´ì ìœ¼ë¡œ ìˆ˜ì§‘Â·ì •ë¦¬í•´ ì£¼ì„¸ìš”. \"\n",
        "        \"í™•ì‹ ë„ í‘œê¸°, ë¶ˆí™•ì‹¤/ê·¼ì‚¬ ëª…ì‹œ. JSONë§Œ ë°˜í™˜.\\n\\n\"\n",
        "        + json.dumps(schema, ensure_ascii=False)\n",
        "        + \"\\n\\ní•˜ìœ„ ì§ˆë¬¸:\\n\" + subq\n",
        "        + f\"\\n\\nì§€ì‹ ì»·ì˜¤í”„: {KNOWLEDGE_CUTOFF}\"\n",
        "    )\n",
        "    out = llm_x.invoke([\n",
        "        SystemMessage(content=STYLE_SYS),\n",
        "        SystemMessage(content=\"ìˆ˜ì¹˜Â·ë‹¨ìœ„Â·ê¸°í˜¸ ë³´ì¡´, ë¶ˆí™•ì‹¤ì„± ëª…ì‹œ, JSON only.\"),\n",
        "        HumanMessage(content=prompt)\n",
        "    ]).content\n",
        "    return jload(out, {\"bullets\": [], \"metrics\": [], \"claims\": [], \"uncertainties\": []})\n",
        "\n",
        "def aggregate(parts: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    bullets, seen = [], set()\n",
        "    metrics, claims, uncertainties = [], [], []\n",
        "    for p in parts:\n",
        "        for b in p.get(\"bullets\", []):\n",
        "            s = b.strip()\n",
        "            if s and s not in seen:\n",
        "                seen.add(s); bullets.append(s)\n",
        "        metrics.extend([m for m in p.get(\"metrics\", []) if isinstance(m, dict)])\n",
        "        claims.extend([c for c in p.get(\"claims\", []) if isinstance(c, dict)])\n",
        "        uncertainties.extend([u for u in p.get(\"uncertainties\", []) if isinstance(u, str)])\n",
        "    return {\n",
        "        \"bullets\": bullets[:14],\n",
        "        \"metrics\": metrics[:12],\n",
        "        \"claims\": claims[:12],\n",
        "        \"uncertainties\": uncertainties[:8]\n",
        "    }\n",
        "\n",
        "def summarize(query: str, agg: Dict[str, Any]) -> str:\n",
        "    prompt = (\n",
        "        \"ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¹´ë“œí˜• ìš”ì•½ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.\\n\"\n",
        "        \"- ì„¹ì…˜ 3~6ê°œ, ê° 3~5ì¤„, ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ì¡´ëŒ“ë§\\n\"\n",
        "        \"- ë‹¨ìœ„/ìˆ˜ì¹˜/ê¸°í˜¸ ë³´ì¡´, ì¶”ì •/ë¶ˆí™•ì‹¤ ëª…ì‹œ\\n\"\n",
        "        f\"- ì§€ì‹ ì»·ì˜¤í”„: {KNOWLEDGE_CUTOFF}\\n\\n\"\n",
        "        f\"ì§ˆë¬¸: {query}\\n\"\n",
        "        f\"í•µì‹¬í¬ì¸íŠ¸: {json.dumps(agg.get('bullets', []), ensure_ascii=False)}\\n\"\n",
        "        f\"ì§€í‘œ: {json.dumps(agg.get('metrics', []), ensure_ascii=False)}\\n\"\n",
        "        f\"ì£¼ì¥: {json.dumps(agg.get('claims', []), ensure_ascii=False)}\\n\"\n",
        "        f\"ë¶ˆí™•ì‹¤: {json.dumps(agg.get('uncertainties', []), ensure_ascii=False)}\\n\"\n",
        "    )\n",
        "    return llm_s.invoke([\n",
        "        SystemMessage(content=STYLE_SYS),\n",
        "        SystemMessage(content=\"ê°„ê²°Â·ì •í™•Â·ê·¼ê±° ìˆ˜ì¤€ í‘œì‹œ\"),\n",
        "        HumanMessage(content=prompt)\n",
        "    ]).content\n",
        "\n",
        "def run(query: str) -> str:\n",
        "    if not query.strip():\n",
        "        return \"ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.\"\n",
        "    # â–¶ NEW: ë¨¼ì € ë‹µë³€ ê°€ëŠ¥ì„± í‰ê°€\n",
        "    ev = evaluate_answerability(query)\n",
        "    if not ev.get(\"answerable\", True):\n",
        "        return guidance_message(ev)\n",
        "    # â–¶ ê°€ëŠ¥í•˜ë©´ ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ ìˆ˜í–‰\n",
        "    subs = make_subquestions(ev.get(\"rewrite_examples\",[query])[0] if ev.get(\"rewrite_examples\") else query)\n",
        "    parts = [extract_self(s) for s in subs]\n",
        "    agg = aggregate(parts)\n",
        "    return summarize(query, agg)\n",
        "\n",
        "def _chat_handler(user_text: str, history: list[list[str]]) -> str:\n",
        "    # ë©€í‹°í„´ ë¬¸ë§¥ì„ ì“°ì§€ ì•Šê³ , ê¸°ì¡´ ë‹¨ì¼í„´ íŒŒì´í”„ë¼ì¸(run) ê·¸ëŒ€ë¡œ í˜¸ì¶œ\n",
        "    return run(user_text)\n",
        "\n",
        "# ê·¸ë¼ë””ì˜¤ UI ìƒ‰ê° ë° ìŠ¤íƒ€ì¼ ë³€ê²½\n",
        "with gr.Blocks(theme=gr.themes.Soft(), title=\"ì „ê¸°Â·ì „ì ìš”ì•½ ì±—ë´‡\") as demo:\n",
        "    gr.HTML(\n",
        "        \"\"\"\n",
        "        <div style=\"text-align: center; max-width: 700px; margin: 0 auto; padding: 20px;\">\n",
        "            <h1 style=\"color: #007bff; font-size: 2.5em; font-weight: bold; margin-bottom: 5px;\">ğŸ”Œ ì „ê¸°Â·ì „ì ìš”ì•½ ì±—ë´‡ ğŸ’¡</h1>\n",
        "            <p style=\"color: #555; font-size: 1.1em;\">ì‹œì¥/ì‚°ì—…/ì—…ê³„ ë™í–¥ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µí•´ ë“œë¦½ë‹ˆë‹¤.</p>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    gr.ChatInterface(\n",
        "        fn=_chat_handler,\n",
        "        chatbot=gr.Chatbot(\n",
        "            height=520,\n",
        "            show_copy_button=True,\n",
        "            render=False, # ìƒìœ„ gr.Blocksì— ì§ì ‘ ë Œë”ë§\n",
        "            layout=\"panel\"\n",
        "        ),\n",
        "        textbox=gr.Textbox(\n",
        "            placeholder=\"ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”. (ì˜ˆ: í•œêµ­ ì¬ìƒì—ë„ˆì§€ ë³´ê¸‰ ë™í–¥ ìš”ì•½)\",\n",
        "            container=False,\n",
        "            scale=7,\n",
        "        ),\n",
        "        examples=[\n",
        "            \"í•œêµ­ ì¬ìƒì—ë„ˆì§€ ë³´ê¸‰ ë™í–¥ ìš”ì•½\",\n",
        "            \"BESS ì‹œì¥ íŠ¸ë Œë“œ í•µì‹¬ í¬ì¸íŠ¸\",\n",
        "            \"EV ì¶©ì „ ì¸í”„ë¼ ìµœê·¼ ì´ìŠˆ ì •ë¦¬\",\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    demo.launch(share=True) # Colabì€ share=True í•„ìš”"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "KyYW53c5yao1",
        "outputId": "843fe7bf-37a6-4564-e7eb-288f143be1f1"
      },
      "id": "KyYW53c5yao1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3019489585.py:213: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot=gr.Chatbot(\n",
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:328: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://eef428fccb1c49a2ab.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://eef428fccb1c49a2ab.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ìµœì¢…ë³¸(2)"
      ],
      "metadata": {
        "id": "_9xV0SFtpQhf"
      },
      "id": "_9xV0SFtpQhf"
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# ì „ê¸°Â·ì „ì ë„ë©”ì¸ ìš”ì•½ ì±—ë´‡ (ì£¼ì œ ë¶„ë¥˜ ê¸°ë°˜ ì‘ë‹µ ì§‘ì¤‘í˜•)\n",
        "\n",
        "# â–¶ ì‚¬ì „ ì¤€ë¹„ (.env íŒŒì¼ ì˜ˆì‹œ)\n",
        "# OPENAI_API_KEY=sk-xxxxx\n",
        "# OPENAI_MODEL=gpt-4o-mini\n",
        "\n",
        "import os\n",
        "import json\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "import gradio as gr\n",
        "\n",
        "# -----------------------------\n",
        "# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ (.env)\n",
        "# -----------------------------\n",
        "load_dotenv()\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    raise RuntimeError(\"OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ ì¤€ë¹„í•˜ê±°ë‚˜ í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ ì£¼ì„¸ìš”.\")\n",
        "\n",
        "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
        "\n",
        "# -----------------------------\n",
        "# LLM ì¸ìŠ¤í„´ìŠ¤\n",
        "# -----------------------------\n",
        "llm_g = ChatOpenAI(model=OPENAI_MODEL, temperature=0)       # ê°€ë“œ/íŒì •/ë¦¬ë¼ì´íŠ¸/ë¶„ë¥˜\n",
        "llm_x = ChatOpenAI(model=OPENAI_MODEL, temperature=0.2)     # ìì²´ ì¶”ì¶œ\n",
        "llm_s = ChatOpenAI(model=OPENAI_MODEL, temperature=0.2)     # ìš”ì•½\n",
        "\n",
        "# -----------------------------\n",
        "# ì •ì±…/ìŠ¤íƒ€ì¼\n",
        "# -----------------------------\n",
        "KNOWLEDGE_CUTOFF = \"2024-06\"\n",
        "\n",
        "STYLE_SYS = (\n",
        "    \"í•œêµ­ì–´ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë§íˆ¬ëŠ” ì¹œì ˆí•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ìœ ì§€í•©ë‹ˆë‹¤. \"\n",
        "    \"í•µì‹¬ì€ ê°„ê²°í•˜ê²Œ ì „ë‹¬í•˜ë˜, ì „ë ¥Â·ì—ë„ˆì§€Â·ëª¨ë¹Œë¦¬í‹° ë¶„ì•¼ì˜ ìˆ˜ì¹˜/ë‹¨ìœ„/ê¸°í˜¸(Î·, THD, pf, pu, kW, kWh, Â°C ë“±)ëŠ” ë³´ì¡´í•©ë‹ˆë‹¤. \"\n",
        "    \"ë¶ˆí™•ì‹¤í•˜ê±°ë‚˜ ê¸°ì–µì´ ëª¨í˜¸í•œ ë‚´ìš©ì€ 'ë¶ˆí™•ì‹¤'ë¡œ í‘œì‹œí•˜ê³  ì¶”ì •Â·ì¼ë°˜ë¡ ì€ ëª…í™•íˆ êµ¬ë¶„í•©ë‹ˆë‹¤. ê³¼ì¥ í‘œí˜„ì€ ì§€ì–‘í•©ë‹ˆë‹¤.\"\n",
        ")\n",
        "\n",
        "# í—ˆìš© ë„ë©”ì¸(ê´‘ë²”ìœ„í•˜ê²Œ í¬í•¨, ì¬ìƒì—ë„ˆì§€ í¸í–¥ ì œê±°)\n",
        "ALLOW = [\n",
        "    \"ì „ë ¥\",\"ì „ë ¥ì‹œì¥\",\"ì „ê¸°\",\"ì „ê¸°ëª¨í„°\",\"ëª¨í„°\",\"ì—ë„ˆì§€\",\"ëª¨ë¹Œë¦¬í‹°\",\"EV\",\"ì¶©ì „\",\"ë°°í„°ë¦¬\",\"BESS\",\n",
        "    \"ì‹ ì¬ìƒ\",\"íƒœì–‘ê´‘\",\"í’ë ¥\",\"ìˆ˜ì†Œ\",\"ì¸ë²„í„°\",\"ì»¨ë²„í„°\",\"PCS\",\"V2G\",\"EMS\",\"DER\",\"DR\",\n",
        "    \"ìŠ¤ë§ˆíŠ¸ê·¸ë¦¬ë“œ\",\"ì†¡ë°°ì „\",\"ë³€ì „\",\"THD\",\"pf\",\"pu\",\"ê³ ì¡°íŒŒ\",\"ì „ì••ì•ˆì •ë„\",\"ì‹ ë¢°ë„\",\"ì‹œì¥\",\"íŠ¸ë Œë“œ\",\"ë™í–¥\"\n",
        "]\n",
        "\n",
        "# ë‹µë³€ ì£¼ì œ ë¼ë²¨\n",
        "TOPIC_LABELS = [\n",
        "    \"ì „ë ¥ì‹œì¥\", \"ì „ê¸°ëª¨í„°\", \"ì‹ ì¬ìƒì—ë„ˆì§€\", \"EV/ì¶©ì „\", \"ë°°í„°ë¦¬/BESS\",\n",
        "    \"ìŠ¤ë§ˆíŠ¸ê·¸ë¦¬ë“œ\", \"ì†¡ë°°ì „/ë³€ì „\", \"ê¸°íƒ€\"\n",
        "]\n",
        "\n",
        "# -----------------------------\n",
        "# ìœ í‹¸ í•¨ìˆ˜\n",
        "# -----------------------------\n",
        "def jload(s: str, default: Any) -> Any:\n",
        "    try:\n",
        "        return json.loads(s)\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "# -----------------------------\n",
        "# ì£¼ì œ ë¶„ë¥˜ê¸°\n",
        "# -----------------------------\n",
        "def classify_topic(q: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    ì§ˆë¬¸ì„ ë¯¸ë¦¬ ì •ì˜í•œ í† í”½ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜.\n",
        "    ì¶œë ¥ ì˜ˆ:\n",
        "    {\n",
        "      \"topic\": \"ì „ë ¥ì‹œì¥\",  # TOPIC_LABELS ì¤‘ í•˜ë‚˜\n",
        "      \"subtopics\": [\"ë„ë§¤ì‹œì¥\", \"ê°€ê²©ì²´ê³„\"],\n",
        "      \"confidence\": \"high|medium|low\",\n",
        "      \"explain\": \"ê°„ë‹¨ ê·¼ê±°\"\n",
        "    }\n",
        "    \"\"\"\n",
        "    schema = {\"topic\": \"\", \"subtopics\": [], \"confidence\": \"medium\", \"explain\": \"\"}\n",
        "    prompt = (\n",
        "        \"ë‹¤ìŒ ì§ˆë¬¸ì„ ì•„ë˜ í† í”½ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë¶„ë¥˜í•˜ê³  JSONìœ¼ë¡œë§Œ ë‹µí•˜ì„¸ìš”.\\n\"\n",
        "        f\"ê°€ëŠ¥í•œ í† í”½: {TOPIC_LABELS}\\n\"\n",
        "        \"ê·œì¹™:\\n\"\n",
        "        \"- ì§ˆë¬¸ì˜ í•µì‹¬ì„ ê°€ì¥ ì˜ ëŒ€í‘œí•˜ëŠ” ë‹¨ í•˜ë‚˜ì˜ topicì„ ê³ ë¥´ì„¸ìš”.\\n\"\n",
        "        \"- subtopicsëŠ” 1~3ê°œ, ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´.\\n\"\n",
        "        \"- confidenceëŠ” high/medium/low ì¤‘ í•˜ë‚˜.\\n\"\n",
        "        \"- explainì—ëŠ” ê°„ë‹¨í•œ ê·¼ê±°ë¥¼ 1ì¤„ë¡œ.\\n\\n\"\n",
        "        f\"ì§ˆë¬¸: {q}\\n\\n\"\n",
        "        f\"ë°˜í™˜ ìŠ¤í‚¤ë§ˆ: {json.dumps(schema, ensure_ascii=False)}\"\n",
        "    )\n",
        "    out = llm_g.invoke([\n",
        "        SystemMessage(content=\"JSON only.\"),\n",
        "        HumanMessage(content=prompt)\n",
        "    ]).content\n",
        "    data = jload(out, schema)\n",
        "    if data.get(\"topic\") not in TOPIC_LABELS:\n",
        "        data[\"topic\"] = \"ê¸°íƒ€\"\n",
        "    if not isinstance(data.get(\"subtopics\"), list):\n",
        "        data[\"subtopics\"] = []\n",
        "    return data\n",
        "\n",
        "# -----------------------------\n",
        "# íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ 1: ë‹µë³€ ê°€ëŠ¥ì„± í‰ê°€\n",
        "# -----------------------------\n",
        "def evaluate_answerability(q: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    JSON ì˜ˆì‹œ\n",
        "    {\n",
        "      \"answerable\": true|false,\n",
        "      \"reasons\": [\"off_topic\",\"requires_web\",\"knowledge_cutoff\",\"ambiguous\",\"too_broad\",\"privacy_sensitive\",\"unsafe\"],\n",
        "      \"explain\": \"ì™œ ë°”ë¡œ ë‹µí•˜ê¸° ì–´ë ¤ìš´ì§€ í•œ ì¤„ ì„¤ëª…(ì¡´ëŒ“ë§)\",\n",
        "      \"ask_for\": [\"ëˆ„ë½ëœ êµ¬ì²´ ì •ë³´ í•­ëª©ë“¤\"],\n",
        "      \"rewrite_examples\": [\"ê¶Œì¥ ì¬ì§ˆë¬¸ 1\",\"ê¶Œì¥ ì¬ì§ˆë¬¸ 2\"]\n",
        "    }\n",
        "    \"\"\"\n",
        "    policy = (\n",
        "        \"ë‹¹ì‹ ì€ ì§ˆì˜ê°€ ì•„ë˜ ì œì•½ì—ì„œ ë‹µë³€ ê°€ëŠ¥í•œì§€ íŒì •í•©ë‹ˆë‹¤.\\n\"\n",
        "        f\"- ì§€ì‹ ì»·ì˜¤í”„: {KNOWLEDGE_CUTOFF}\\n\"\n",
        "        \"- ì™¸ë¶€ ì›¹ ê²€ìƒ‰/ì‹¤ì‹œê°„ ë°ì´í„° ì‚¬ìš© ë¶ˆê°€\\n\"\n",
        "        \"- ë„ë©”ì¸: ì „ë ¥Â·ì—ë„ˆì§€Â·ëª¨ë¹Œë¦¬í‹° ì¤‘ì‹¬\\n\"\n",
        "        \"- ê°œì¸ì‹ë³„/ë¯¼ê°ì •ë³´, ë¶ˆë²•/ìœ„í—˜, ì˜ë£ŒÂ·ë²•ë¥ Â·íˆ¬ì ê°œë³„ ì¡°ì–¸ì€ ë¶ˆê°€\\n\"\n",
        "        \"- ëª¨í˜¸/ê³¼ë„í•˜ê²Œ ê´‘ë²”ìœ„/ì •ì˜ê°€ ë¶ˆëª…í™•í•˜ë©´ êµ¬ì²´í™”ê°€ í•„ìš”\\n\"\n",
        "        \"ìœ„ ì¡°ê±´ì„ ë°”íƒ•ìœ¼ë¡œ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.\"\n",
        "    )\n",
        "    schema = {\n",
        "        \"answerable\": True,\n",
        "        \"reasons\": [],\n",
        "        \"explain\": \"\",\n",
        "        \"ask_for\": [],\n",
        "        \"rewrite_examples\": []\n",
        "    }\n",
        "    prompt = (\n",
        "        policy + \"\\n\\n\"\n",
        "        \"ê°€ëŠ¥í•˜ë©´ ì‚¬ìš©ìê°€ ë°”ë¡œ ì“¸ ìˆ˜ ìˆëŠ” 'ì¬ì§ˆë¬¸ ì˜ˆì‹œ'ë¥¼ 1~3ê°œ ì œì•ˆí•˜ì„¸ìš” \"\n",
        "        \"(ì „ë ¥Â·ì—ë„ˆì§€ ë§¥ë½ì˜ ì§€í‘œ/ë²”ìœ„/ì§€ì—­/ê¸°ê°„/ëŒ€ìƒ ë“±ì„ í¬í•¨). \"\n",
        "        \"JSONë§Œ ì¶œë ¥. ìŠ¤í‚¤ë§ˆëŠ” ë‹¤ìŒê³¼ ê°™ìŒ:\\n\"\n",
        "        + json.dumps(schema, ensure_ascii=False) + \"\\n\\n\"\n",
        "        f\"ì§ˆë¬¸: {q}\"\n",
        "    )\n",
        "    out = llm_g.invoke([\n",
        "        SystemMessage(content=STYLE_SYS),\n",
        "        SystemMessage(content=\"JSON only.\"),\n",
        "        HumanMessage(content=prompt)\n",
        "    ]).content\n",
        "    data = jload(out, schema)\n",
        "\n",
        "    # íœ´ë¦¬ìŠ¤í‹± ë³´ê°•: ì‹¤ì‹œê°„/ìµœì‹ /ê°€ê²©/ì£¼ê°€/ë‚ ì”¨ ë“±\n",
        "    recent_triggers = [\"ì‹¤ì‹œê°„\",\"í˜„ì¬\",\"ì˜¤ëŠ˜\",\"ë°©ê¸ˆ\",\"ì§€ê¸ˆ\",\"ìµœì‹ \",\"ì£¼ê°€\",\"í™˜ìœ¨\",\"ë‚ ì”¨\",\"ìŠ¤ì½”ì–´\",\"ì†ë³´\",\"ë¼ì´ë¸Œ\"]\n",
        "    if any(t in q for t in recent_triggers):\n",
        "        data[\"answerable\"] = False\n",
        "        if \"requires_web\" not in data[\"reasons\"]:\n",
        "            data[\"reasons\"].append(\"requires_web\")\n",
        "        data[\"explain\"] = data.get(\"explain\") or \"ì‹¤ì‹œê°„Â·ìµœì‹  ë°ì´í„°ëŠ” ì›¹ ì ‘ê·¼ ì—†ì´ ì •í™•íˆ ì•ˆë‚´ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "    # ë„ë©”ì¸ ì²´í¬(ê´‘ë²”ìœ„ í¬í•¨)\n",
        "    if not any(k.lower() in q.lower() for k in ALLOW):\n",
        "        if \"off_topic\" not in data[\"reasons\"]:\n",
        "            data[\"reasons\"].append(\"off_topic\")\n",
        "    return data\n",
        "\n",
        "# -----------------------------\n",
        "# íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ 1-ë³´ì¡°: ê°€ì´ë“œ ë¬¸êµ¬ ìƒì„±\n",
        "# -----------------------------\n",
        "def guidance_message(evald: Dict[str, Any]) -> str:\n",
        "    reasons_ko = {\n",
        "        \"off_topic\": \"ì¤€ë¹„ëœ ì£¼ì œ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤.\",\n",
        "        \"requires_web\": \"ì‹¤ì‹œê°„/ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•œ ì£¼ì œì…ë‹ˆë‹¤.\",\n",
        "        \"knowledge_cutoff\": f\"ì§€ì‹ ì»·ì˜¤í”„({KNOWLEDGE_CUTOFF}) ì´í›„ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.\",\n",
        "        \"ambiguous\": \"ì§ˆë¬¸ì´ ëª¨í˜¸í•©ë‹ˆë‹¤.\",\n",
        "        \"too_broad\": \"ì§ˆë¬¸ ë²”ìœ„ê°€ ë„ˆë¬´ ë„“ìŠµë‹ˆë‹¤.\",\n",
        "        \"privacy_sensitive\": \"ê°œì¸ì •ë³´Â·ë¯¼ê°ì •ë³´ë¼ì„œ ë„ì™€ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤.\",\n",
        "        \"unsafe\": \"ìœ„í—˜/ë¶€ì ì ˆí•œ ìš”ì²­ì…ë‹ˆë‹¤.\"\n",
        "    }\n",
        "    lines = [\"ìš”ì²­í•˜ì‹  ë‚´ìš©ì€ ë°”ë¡œ ì •í™•íˆ ë‹µë³€ë“œë¦¬ê¸° ì–´ë µìŠµë‹ˆë‹¤.\"]\n",
        "    if evald.get(\"explain\"):\n",
        "        lines.append(f\"ì‚¬ìœ : {evald['explain']}\")\n",
        "    if evald.get(\"reasons\"):\n",
        "        labs = [reasons_ko.get(r, r) for r in evald[\"reasons\"]]\n",
        "        lines.append(\"íŒì •: \" + \", \".join(labs))\n",
        "    if evald.get(\"ask_for\"):\n",
        "        lines.append(\"\\në‹¤ìŒ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ë” ì •í™•íˆ ë„ì™€ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\")\n",
        "        lines += [f\"- {a}\" for a in evald[\"ask_for\"][:6]]\n",
        "    if evald.get(\"rewrite_examples\"):\n",
        "        lines.append(\"\\nì´ë ‡ê²Œ ì§ˆë¬¸í•´ ë³´ì‹œë©´ ì¢‹ìŠµë‹ˆë‹¤:\")\n",
        "        lines += [f\"- {r}\" for r in evald[\"rewrite_examples\"][:3]]\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "# -----------------------------\n",
        "# íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ 2: í•˜ìœ„ ì§ˆë¬¸ ë¶„í•´ (ì£¼ì œ ê³ ì •)\n",
        "# -----------------------------\n",
        "def make_subquestions(q: str, topic: str) -> List[str]:\n",
        "    prompt = (\n",
        "        \"ë‹¤ìŒ ì§ˆë¬¸ì„ 3~5ê°œì˜ í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´í•˜ì„¸ìš”.\\n\"\n",
        "        f\"- ë¶„ë¥˜ëœ ì£¼ì œì—ë§Œ ì§‘ì¤‘í•˜ì—¬ ì‘ì„±: {topic}\\n\"\n",
        "        \"- ìµœì‹ ì„± ì´ìŠˆê°€ ìˆìœ¼ë©´ 'ìµœê·¼ ë™í–¥ í™•ì¸'ì„ í¬í•¨í•©ë‹ˆë‹¤.\\n\"\n",
        "        'JSONë§Œ ì¶œë ¥: {\"subs\": [\"...\"]}\\n\\n'\n",
        "        f\"ì§ˆë¬¸: {q}\"\n",
        "    )\n",
        "    out = llm_x.invoke([SystemMessage(content=\"JSON only.\"), HumanMessage(content=prompt)]).content\n",
        "    data = jload(out, {\"subs\":[q]})\n",
        "    subs = [s for s in data.get(\"subs\", []) if isinstance(s, str) and s.strip()]\n",
        "    return subs[:5] if subs else [q]\n",
        "\n",
        "# -----------------------------\n",
        "# íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ 3: ìì²´ ì •ë³´ ì¶”ì¶œ (ì£¼ì œ ê³ ì •)\n",
        "# -----------------------------\n",
        "def extract_self(subq: str, topic: str, subtopics: List[str]) -> Dict[str, Any]:\n",
        "    schema = {\n",
        "        \"bullets\": [],\n",
        "        \"metrics\": [{\"name\": \"\", \"value\": \"\", \"unit\": \"\", \"context\": \"\", \"confidence\": \"low|medium|high\"}],\n",
        "        \"claims\": [{\"statement\": \"\", \"confidence\": \"low|medium|high\"}],\n",
        "        \"uncertainties\": []\n",
        "    }\n",
        "    prompt = (\n",
        "        \"ì™¸ë¶€ ê²€ìƒ‰ ì—†ì´, ì¼ë°˜ ì§€ì‹ ë²”ìœ„ì—ì„œ ì•„ë˜ í•˜ìœ„ ì§ˆë¬¸ì˜ í•µì‹¬ ì •ë³´ë¥¼ ìì²´ì ìœ¼ë¡œ ìˆ˜ì§‘Â·ì •ë¦¬í•´ ì£¼ì„¸ìš”.\\n\"\n",
        "        f\"- ë¶„ë¥˜ëœ ì£¼ì œ: {topic}\\n\"\n",
        "        f\"- ì„œë¸Œí† í”½(ì°¸ê³ ): {', '.join(subtopics) if subtopics else 'ì—†ìŒ'}\\n\"\n",
        "        \"- ë¶„ë¥˜ëœ ì£¼ì œì™€ ë¬´ê´€í•œ ë¶„ì•¼(ì˜ˆ: ë‹¤ë¥¸ ì‚°ì—…/ê¸°ìˆ )ëŠ” ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.\\n\"\n",
        "        \"- í™•ì‹ ë„ í‘œê¸°, ë¶ˆí™•ì‹¤/ê·¼ì‚¬ ëª…ì‹œ. JSONë§Œ ë°˜í™˜.\\n\\n\"\n",
        "        + json.dumps(schema, ensure_ascii=False)\n",
        "        + \"\\n\\ní•˜ìœ„ ì§ˆë¬¸:\\n\" + subq\n",
        "        + f\"\\n\\nì§€ì‹ ì»·ì˜¤í”„: {KNOWLEDGE_CUTOFF}\"\n",
        "    )\n",
        "    out = llm_x.invoke([\n",
        "        SystemMessage(content=STYLE_SYS),\n",
        "        SystemMessage(content=\"ìˆ˜ì¹˜Â·ë‹¨ìœ„Â·ê¸°í˜¸ ë³´ì¡´, ë¶ˆí™•ì‹¤ì„± ëª…ì‹œ, JSON only.\"),\n",
        "        HumanMessage(content=prompt)\n",
        "    ]).content\n",
        "    return jload(out, {\"bullets\": [], \"metrics\": [], \"claims\": [], \"uncertainties\": []})\n",
        "\n",
        "# -----------------------------\n",
        "# íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ 4: í†µí•©\n",
        "# -----------------------------\n",
        "def aggregate(parts: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    bullets, seen = [], set()\n",
        "    metrics, claims, uncertainties = [], [], []\n",
        "    for p in parts:\n",
        "        for b in p.get(\"bullets\", []):\n",
        "            s = b.strip()\n",
        "            if s and s not in seen:\n",
        "                seen.add(s); bullets.append(s)\n",
        "        metrics.extend([m for m in p.get(\"metrics\", []) if isinstance(m, dict)])\n",
        "        claims.extend([c for c in p.get(\"claims\", []) if isinstance(c, dict)])\n",
        "        uncertainties.extend([u for u in p.get(\"uncertainties\", []) if isinstance(u, str)])\n",
        "    return {\n",
        "        \"bullets\": bullets[:14],\n",
        "        \"metrics\": metrics[:12],\n",
        "        \"claims\": claims[:12],\n",
        "        \"uncertainties\": uncertainties[:8]\n",
        "    }\n",
        "\n",
        "# -----------------------------\n",
        "# íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ 5: ìš”ì•½ ìƒì„± (ì£¼ì œ ê³ ì •)\n",
        "# -----------------------------\n",
        "def summarize(query: str, agg: Dict[str, Any], topic: str, subtopics: List[str]) -> str:\n",
        "    topic_rule = (\n",
        "        f\"ì´ë²ˆ ì‘ë‹µì€ ë°˜ë“œì‹œ '{topic}' ì£¼ì œì—ë§Œ ì§‘ì¤‘í•´ì•¼ í•©ë‹ˆë‹¤. \"\n",
        "        \"ë¶„ë¥˜ëœ ì£¼ì œì™€ ì§ì ‘ì  ê´€ë ¨ì´ ì—†ëŠ” íƒ€ ë¶„ì•¼(ì˜ˆ: ë‹¤ë¥¸ ì—ë„ˆì§€ ê¸°ìˆ /ì‚°ì—…)ëŠ” ì–¸ê¸‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\"\n",
        "    )\n",
        "    prompt = (\n",
        "        \"ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¹´ë“œí˜• ìš”ì•½ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.\\n\"\n",
        "        \"- ì„¹ì…˜ 3~6ê°œ, ê° 3~5ì¤„, ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ì¡´ëŒ“ë§\\n\"\n",
        "        \"- ë‹¨ìœ„/ìˆ˜ì¹˜/ê¸°í˜¸ ë³´ì¡´, ì¶”ì •/ë¶ˆí™•ì‹¤ ëª…ì‹œ\\n\"\n",
        "        f\"- ì§€ì‹ ì»·ì˜¤í”„: {KNOWLEDGE_CUTOFF}\\n\"\n",
        "        f\"- ì£¼ì œ ê·œì¹™: {topic_rule}\\n\"\n",
        "        f\"- ì„œë¸Œí† í”½(ì°¸ê³ ): {', '.join(subtopics) if subtopics else 'ì—†ìŒ'}\\n\\n\"\n",
        "        f\"ì§ˆë¬¸: {query}\\n\"\n",
        "        f\"í•µì‹¬í¬ì¸íŠ¸: {json.dumps(agg.get('bullets', []), ensure_ascii=False)}\\n\"\n",
        "        f\"ì§€í‘œ: {json.dumps(agg.get('metrics', []), ensure_ascii=False)}\\n\"\n",
        "        f\"ì£¼ì¥: {json.dumps(agg.get('claims', []), ensure_ascii=False)}\\n\"\n",
        "        f\"ë¶ˆí™•ì‹¤: {json.dumps(agg.get('uncertainties', []), ensure_ascii=False)}\\n\"\n",
        "    )\n",
        "    return llm_s.invoke([\n",
        "        SystemMessage(content=STYLE_SYS),\n",
        "        SystemMessage(content=\"ê°„ê²°Â·ì •í™•Â·ê·¼ê±° ìˆ˜ì¤€ í‘œì‹œ\"),\n",
        "        HumanMessage(content=prompt)\n",
        "    ]).content\n",
        "\n",
        "# -----------------------------\n",
        "# ì‹¤í–‰ í•¸ë“¤ëŸ¬\n",
        "# -----------------------------\n",
        "def run(query: str) -> str:\n",
        "    if not query.strip():\n",
        "        return \"ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.\"\n",
        "    # 0) ì£¼ì œ ë¶„ë¥˜\n",
        "    cls = classify_topic(query)\n",
        "    topic = cls.get(\"topic\", \"ê¸°íƒ€\")\n",
        "    subtopics = cls.get(\"subtopics\", [])\n",
        "    # 1) ë‹µë³€ ê°€ëŠ¥ì„± í‰ê°€\n",
        "    ev = evaluate_answerability(query)\n",
        "    if not ev.get(\"answerable\", True):\n",
        "        # ê°€ì´ë“œ ë©”ì‹œì§€ì— ì£¼ì œ ì •ë³´ ë³´ê°•\n",
        "        guide = guidance_message(ev)\n",
        "        guide += f\"\\n\\n[ë¶„ë¥˜ ê²°ê³¼] ì£¼ì œ: {topic}, ì‹ ë¢°ë„: {cls.get('confidence','medium')}\"\n",
        "        return guide\n",
        "    # 2) í•˜ìœ„ ì§ˆë¬¸ ë¶„í•´ (ì£¼ì œ ê³ ì •)\n",
        "    base_q = ev.get(\"rewrite_examples\", [query])[0] if ev.get(\"rewrite_examples\") else query\n",
        "    subs = make_subquestions(base_q, topic)\n",
        "    # 3) ìì²´ ì¶”ì¶œ\n",
        "    parts = [extract_self(s, topic, subtopics) for s in subs]\n",
        "    # 4) í†µí•©\n",
        "    agg = aggregate(parts)\n",
        "    # 5) ìš”ì•½ ìƒì„± (ì£¼ì œ ê³ ì •)\n",
        "    return summarize(query, agg, topic, subtopics)\n",
        "\n",
        "def _chat_handler(user_text: str, history: list[list[str]]) -> str:\n",
        "    # ë©€í‹°í„´ ë¬¸ë§¥ì„ ì“°ì§€ ì•Šê³ , ê¸°ì¡´ ë‹¨ì¼í„´ íŒŒì´í”„ë¼ì¸(run) ê·¸ëŒ€ë¡œ í˜¸ì¶œ\n",
        "    return run(user_text)\n",
        "\n",
        "# -----------------------------\n",
        "# Gradio UI\n",
        "# -----------------------------\n",
        "with gr.Blocks(theme=gr.themes.Soft(), title=\"ì „ê¸°Â·ì „ì ìš”ì•½ ì±—ë´‡(ì£¼ì œ ë¶„ë¥˜í˜•)\") as demo:\n",
        "    gr.HTML(\n",
        "        \"\"\"\n",
        "        <div style=\"text-align: center; max-width: 700px; margin: 0 auto; padding: 20px;\">\n",
        "            <h1 style=\"color: #007bff; font-size: 2.2em; font-weight: bold; margin-bottom: 5px;\">ğŸ”Œ ì „ê¸°Â·ì „ì ìš”ì•½ ì±—ë´‡ ğŸ’¡</h1>\n",
        "            <p style=\"color: #555; font-size: 1.02em;\">ì§ˆë¬¸ì„ ìë™ ë¶„ë¥˜(ì „ë ¥ì‹œì¥/ì „ê¸°ëª¨í„°/ì‹ ì¬ìƒ/EVÂ·ì¶©ì „/ë°°í„°ë¦¬Â·BESS/ìŠ¤ë§ˆíŠ¸ê·¸ë¦¬ë“œ/ì†¡ë°°ì „Â·ë³€ì „)í•˜ì—¬ ì£¼ì œì— ë§ê²Œ ìš”ì•½í•©ë‹ˆë‹¤.</p>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    gr.ChatInterface(\n",
        "        fn=_chat_handler,\n",
        "        chatbot=gr.Chatbot(\n",
        "            height=520,\n",
        "            show_copy_button=True,\n",
        "            render=False, # ìƒìœ„ gr.Blocksì— ì§ì ‘ ë Œë”ë§\n",
        "            layout=\"panel\"\n",
        "        ),\n",
        "        textbox=gr.Textbox(\n",
        "            placeholder=\"ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”. (ì˜ˆ: í•œêµ­ ë„ë§¤ ì „ë ¥ì‹œì¥ ê°€ê²© ê²°ì • êµ¬ì¡° ìš”ì•½)\",\n",
        "            container=False,\n",
        "            scale=7,\n",
        "        ),\n",
        "        examples=[\n",
        "            \"í•œêµ­ ë„ë§¤ ì „ë ¥ì‹œì¥ ê°€ê²© ê²°ì • êµ¬ì¡° ìš”ì•½\",\n",
        "            \"ì˜êµ¬ìì„ ë™ê¸°ëª¨í„°(PMSM) ê³ íš¨ìœ¨ ì„¤ê³„ í•µì‹¬\",\n",
        "            \"íƒœì–‘ê´‘ LCOE êµ¬ì„±ìš”ì†Œì™€ ë¯¼ê°ë„\",\n",
        "            \"DC ê¸‰ì†ì¶©ì „ ì¸í”„ë¼ ê³¼ì œ\",\n",
        "            \"BESS ì‚¬ì´í´ ìˆ˜ëª…ê³¼ ì—´ê´€ë¦¬ ì´ìŠˆ\",\n",
        "            \"ë¶„ì‚°ìì› ì—°ê³„í˜• ìŠ¤ë§ˆíŠ¸ê·¸ë¦¬ë“œ ìš”ì†Œ\",\n",
        "            \"ì†¡ë°°ì „ë§ì—ì„œ ì „ì••ì•ˆì •ë„ ì§€í‘œ ì •ë¦¬\",\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    demo.launch(share=True)  # Colabì€ share=True í•„ìš”\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "nI5oMSKunYTW",
        "outputId": "0fface0d-4510-4de8-bc99-d984c44f1ec7"
      },
      "id": "nI5oMSKunYTW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3716230287.py:326: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot=gr.Chatbot(\n",
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:328: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://30f94fa28a429f7662.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://30f94fa28a429f7662.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}