{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aebonlee/chatbot_01team/blob/main/ICT_%EB%B9%84%EC%A0%84%EA%B3%B5%EC%9E%901%EC%A1%B0_ver_1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ac32751",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ac32751",
        "outputId": "fbd88807-cbbd-4b1d-db21-58668fe00931",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.31-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.74)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.14)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.100.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Downloading langchain_openai-0.3.31-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-openai\n",
            "Successfully installed langchain-openai-0.3.31\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-openai tiktoken python-dotenv\n",
        "#!pip install orjson\n",
        "#!pip install gradio==4.*\n",
        "#!pip install trafilatura"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OlbCnIMa9df8"
      },
      "id": "OlbCnIMa9df8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "471e96aa",
      "metadata": {
        "id": "471e96aa"
      },
      "outputs": [],
      "source": [
        "import os, json\n",
        "from typing import List, Dict, Any\n",
        "import gradio as gr\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import SystemMessage, HumanMessage"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "멋진 UI 전"
      ],
      "metadata": {
        "id": "mUj-dUabywK_"
      },
      "id": "mUj-dUabywK_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7eb0eea4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "7eb0eea4",
        "outputId": "02e2b9d4-21fa-47eb-c501-7fb1fd6efaf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3431024483.py:195: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot=gr.Chatbot(height=520, show_copy_button=True),\n",
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:328: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://9aa7893cb3c115a885.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9aa7893cb3c115a885.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# API키 및 모델 설정\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "OPENAI_MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
        "llm_g = ChatOpenAI(model=MODEL, temperature=0)     # 가드/판정/리라이트\n",
        "llm_x = ChatOpenAI(model=MODEL, temperature=0.2)   # 자체 추출\n",
        "llm_s = ChatOpenAI(model=MODEL, temperature=0.2)   # 요약\n",
        "\n",
        "KNOWLEDGE_CUTOFF = \"2024-06\"\n",
        "\n",
        "STYLE_SYS = (\n",
        "    \"한국어 존댓말을 사용합니다. 말투는 친절하고 전문적으로 유지합니다. \"\n",
        "    \"핵심은 간결하게 전달하되, 전력·에너지·모빌리티 분야의 수치/단위/기호(η, THD, pf, pu, kW, kWh, °C 등)는 보존합니다. \"\n",
        "    \"불확실하거나 기억이 모호한 내용은 '불확실'로 표시하고 추정·일반론은 명확히 구분합니다. 과장 표현은 지양합니다.\"\n",
        ")\n",
        "\n",
        "ALLOW = [\n",
        "    \"전력\",\"에너지\",\"모빌리티\",\"EV\",\"충전\",\"배터리\",\"신재생\",\"태양광\",\"풍력\",\"수소\",\n",
        "    \"인버터\",\"컨버터\",\"PCS\",\"BESS\",\"V2G\",\"EMS\",\"DER\",\"DR\",\"스마트그리드\",\"송배전\",\"변전\",\n",
        "    \"THD\",\"pf\",\"pu\",\"고조파\",\"전압안정도\",\"신뢰도\",\"시장\",\"트렌드\",\"동향\"\n",
        "]\n",
        "\n",
        "def jload(s: str, default: Any) -> Any:\n",
        "    try:\n",
        "        return json.loads(s)\n",
        "    except:\n",
        "        return default\n",
        "\n",
        "def evaluate_answerability(q: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    JSON 예시\n",
        "    {\n",
        "      \"answerable\": true|false,\n",
        "      \"reasons\": [\"off_topic\",\"requires_web\",\"knowledge_cutoff\",\"ambiguous\",\"too_broad\",\"privacy_sensitive\",\"unsafe\"],\n",
        "      \"explain\": \"왜 바로 답하기 어려운지 한 줄 설명(존댓말)\",\n",
        "      \"ask_for\": [\"누락된 구체 정보 항목들\"],\n",
        "      \"rewrite_examples\": [\"권장 재질문 1\",\"권장 재질문 2\"]\n",
        "    }\n",
        "    \"\"\"\n",
        "    policy = (\n",
        "        \"당신은 질의가 아래 제약에서 답변 가능한지 판정합니다.\\n\"\n",
        "        f\"- 지식 컷오프: {KNOWLEDGE_CUTOFF}\\n\"\n",
        "        \"- 외부 웹 검색/실시간 데이터 사용 불가\\n\"\n",
        "        \"- 도메인: 전력·에너지·모빌리티 중심\\n\"\n",
        "        \"- 개인식별/민감정보, 불법/위험, 의료·법률·투자 개별 조언은 불가\\n\"\n",
        "        \"- 모호/과도하게 광범위/정의가 불명확하면 구체화가 필요\\n\"\n",
        "        \"위 조건을 바탕으로 JSON만 반환하세요.\"\n",
        "    )\n",
        "    schema = {\n",
        "        \"answerable\": True,\n",
        "        \"reasons\": [],\n",
        "        \"explain\": \"\",\n",
        "        \"ask_for\": [],\n",
        "        \"rewrite_examples\": []\n",
        "    }\n",
        "    prompt = (\n",
        "        policy + \"\\n\\n\"\n",
        "        \"가능하면 사용자가 바로 쓸 수 있는 '재질문 예시'를 1~3개 제안하세요 \"\n",
        "        \"(전력·에너지 맥락의 지표/범위/지역/기간/대상 등을 포함). \"\n",
        "        \"JSON만 출력. 스키마는 다음과 같음:\\n\"\n",
        "        + json.dumps(schema, ensure_ascii=False) + \"\\n\\n\"\n",
        "        f\"질문: {q}\"\n",
        "    )\n",
        "    out = llm_g.invoke([\n",
        "        SystemMessage(content=STYLE_SYS),\n",
        "        SystemMessage(content=\"JSON only.\"),\n",
        "        HumanMessage(content=prompt)\n",
        "    ]).content\n",
        "    data = jload(out, schema)\n",
        "    # 휴리스틱 보강: 실시간/최신/가격/주가/날씨 등\n",
        "    recent_triggers = [\"실시간\",\"현재\",\"오늘\",\"방금\",\"지금\",\"최신\",\"주가\",\"환율\",\"날씨\",\"스코어\",\"속보\",\"라이브\"]\n",
        "    if any(t in q for t in recent_triggers):\n",
        "        data[\"answerable\"] = False\n",
        "        if \"requires_web\" not in data[\"reasons\"]:\n",
        "            data[\"reasons\"].append(\"requires_web\")\n",
        "        data[\"explain\"] = data.get(\"explain\") or \"실시간·최신 데이터는 웹 접근 없이 정확히 안내드리기 어렵습니다.\"\n",
        "    # 도메인 체크\n",
        "    if not any(k.lower() in q.lower() for k in ALLOW):\n",
        "        if \"off_topic\" not in data[\"reasons\"]:\n",
        "            data[\"reasons\"].append(\"off_topic\")\n",
        "    return data\n",
        "\n",
        "def guidance_message(evald: Dict[str, Any]) -> str:\n",
        "    reasons_ko = {\n",
        "        \"off_topic\": \"준비된 주제 범위를 벗어났습니다.\",\n",
        "        \"requires_web\": \"실시간/웹 검색이 필요한 주제입니다.\",\n",
        "        \"knowledge_cutoff\": f\"지식 컷오프({KNOWLEDGE_CUTOFF}) 이후 정보가 필요합니다.\",\n",
        "        \"ambiguous\": \"질문이 모호합니다.\",\n",
        "        \"too_broad\": \"질문 범위가 너무 넓습니다.\",\n",
        "        \"privacy_sensitive\": \"개인정보·민감정보라서 도와드리기 어렵습니다.\",\n",
        "        \"unsafe\": \"위험/부적절한 요청입니다.\"\n",
        "    }\n",
        "    lines = [\"요청하신 내용은 바로 정확히 답변드리기 어렵습니다.\"]\n",
        "    if evald.get(\"explain\"):\n",
        "        lines.append(f\"사유: {evald['explain']}\")\n",
        "    if evald.get(\"reasons\"):\n",
        "        labs = [reasons_ko.get(r, r) for r in evald[\"reasons\"]]\n",
        "        lines.append(\"판정: \" + \", \".join(labs))\n",
        "    if evald.get(\"ask_for\"):\n",
        "        lines.append(\"\\n다음 정보를 알려주시면 더 정확히 도와드릴 수 있습니다:\")\n",
        "        lines += [f\"- {a}\" for a in evald[\"ask_for\"][:6]]\n",
        "    if evald.get(\"rewrite_examples\"):\n",
        "        lines.append(\"\\n이렇게 질문해 보시면 좋습니다:\")\n",
        "        lines += [f\"- {r}\" for r in evald[\"rewrite_examples\"][:3]]\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def make_subquestions(q: str) -> List[str]:\n",
        "    prompt = (\n",
        "        \"다음 질문을 3~5개의 하위 질문으로 분해하세요. 최신성 이슈가 있으면 '최근 동향 확인'을 포함합니다. \"\n",
        "        \"JSON만 출력: {\\\"subs\\\": [\\\"...\\\"]}\\n\\n\"\n",
        "        f\"질문: {q}\"\n",
        "    )\n",
        "    out = llm_x.invoke([SystemMessage(content=\"JSON only.\"), HumanMessage(content=prompt)]).content\n",
        "    data = jload(out, {\"subs\":[q]})\n",
        "    subs = [s for s in data.get(\"subs\", []) if isinstance(s, str) and s.strip()]\n",
        "    return subs[:5] if subs else [q]\n",
        "\n",
        "def extract_self(subq: str) -> Dict[str, Any]:\n",
        "    schema = {\n",
        "        \"bullets\": [],\n",
        "        \"metrics\": [{\"name\": \"\", \"value\": \"\", \"unit\": \"\", \"context\": \"\", \"confidence\": \"low|medium|high\"}],\n",
        "        \"claims\": [{\"statement\": \"\", \"confidence\": \"low|medium|high\"}],\n",
        "        \"uncertainties\": []\n",
        "    }\n",
        "    prompt = (\n",
        "        \"외부 검색 없이, 일반 지식 범위에서 아래 하위 질문의 핵심 정보를 자체적으로 수집·정리해 주세요. \"\n",
        "        \"확신도 표기, 불확실/근사는 명시. JSON만 반환.\\n\\n\"\n",
        "        + json.dumps(schema, ensure_ascii=False)\n",
        "        + \"\\n\\n하위 질문:\\n\" + subq\n",
        "        + f\"\\n\\n지식 컷오프: {KNOWLEDGE_CUTOFF}\"\n",
        "    )\n",
        "    out = llm_x.invoke([\n",
        "        SystemMessage(content=STYLE_SYS),\n",
        "        SystemMessage(content=\"수치·단위·기호 보존, 불확실성 명시, JSON only.\"),\n",
        "        HumanMessage(content=prompt)\n",
        "    ]).content\n",
        "    return jload(out, {\"bullets\": [], \"metrics\": [], \"claims\": [], \"uncertainties\": []})\n",
        "\n",
        "def aggregate(parts: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    bullets, seen = [], set()\n",
        "    metrics, claims, uncertainties = [], [], []\n",
        "    for p in parts:\n",
        "        for b in p.get(\"bullets\", []):\n",
        "            s = b.strip()\n",
        "            if s and s not in seen:\n",
        "                seen.add(s); bullets.append(s)\n",
        "        metrics.extend([m for m in p.get(\"metrics\", []) if isinstance(m, dict)])\n",
        "        claims.extend([c for c in p.get(\"claims\", []) if isinstance(c, dict)])\n",
        "        uncertainties.extend([u for u in p.get(\"uncertainties\", []) if isinstance(u, str)])\n",
        "    return {\n",
        "        \"bullets\": bullets[:14],\n",
        "        \"metrics\": metrics[:12],\n",
        "        \"claims\": claims[:12],\n",
        "        \"uncertainties\": uncertainties[:8]\n",
        "    }\n",
        "\n",
        "def summarize(query: str, agg: Dict[str, Any]) -> str:\n",
        "    prompt = (\n",
        "        \"다음 정보를 바탕으로 카드형 요약을 작성해 주세요.\\n\"\n",
        "        \"- 섹션 3~6개, 각 3~5줄, 친절하고 전문적인 존댓말\\n\"\n",
        "        \"- 단위/수치/기호 보존, 추정/불확실 명시\\n\"\n",
        "        f\"- 지식 컷오프: {KNOWLEDGE_CUTOFF}\\n\\n\"\n",
        "        f\"질문: {query}\\n\"\n",
        "        f\"핵심포인트: {json.dumps(agg.get('bullets', []), ensure_ascii=False)}\\n\"\n",
        "        f\"지표: {json.dumps(agg.get('metrics', []), ensure_ascii=False)}\\n\"\n",
        "        f\"주장: {json.dumps(agg.get('claims', []), ensure_ascii=False)}\\n\"\n",
        "        f\"불확실: {json.dumps(agg.get('uncertainties', []), ensure_ascii=False)}\\n\"\n",
        "    )\n",
        "    return llm_s.invoke([\n",
        "        SystemMessage(content=STYLE_SYS),\n",
        "        SystemMessage(content=\"간결·정확·근거 수준 표시\"),\n",
        "        HumanMessage(content=prompt)\n",
        "    ]).content\n",
        "\n",
        "def run(query: str) -> str:\n",
        "    if not query.strip():\n",
        "        return \"질문을 입력해 주세요.\"\n",
        "    # ▶ NEW: 먼저 답변 가능성 평가\n",
        "    ev = evaluate_answerability(query)\n",
        "    if not ev.get(\"answerable\", True):\n",
        "        return guidance_message(ev)\n",
        "    # ▶ 가능하면 기존 파이프라인 수행\n",
        "    subs = make_subquestions(ev.get(\"rewrite_examples\",[query])[0] if ev.get(\"rewrite_examples\") else query)\n",
        "    parts = [extract_self(s) for s in subs]\n",
        "    agg = aggregate(parts)\n",
        "    return summarize(query, agg)\n",
        "\n",
        "def _chat_handler(user_text: str, history: list[list[str]]) -> str:\n",
        "    # 멀티턴 문맥을 쓰지 않고, 기존 단일턴 파이프라인(run) 그대로 호출\n",
        "    return run(user_text)\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    fn=_chat_handler,\n",
        "    chatbot=gr.Chatbot(height=520, show_copy_button=True),\n",
        "    title=\"전력·에너지 요약 챗봇\",\n",
        "    description=\"시장/산업/업계 동향 관련 질문에 답합니다.\",\n",
        "    examples=[\n",
        "        \"한국 재생에너지 보급 동향 요약\",\n",
        "        \"BESS 시장 트렌드 핵심 포인트\",\n",
        "        \"EV 충전 인프라 최근 이슈 정리\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "demo.launch(share=True)  # Colab은 share=True 필요"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "더 이쁜 UI(최종)"
      ],
      "metadata": {
        "id": "1F2CM1_yys4a"
      },
      "id": "1F2CM1_yys4a"
    },
    {
      "cell_type": "code",
      "source": [
        "# API키 및 모델 설정\n",
        "import os\n",
        "import json\n",
        "from typing import Any, Dict, List\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "import gradio as gr\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "OPENAI_MODEL = \"gpt-4o-mini\"\n",
        "\n",
        "MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
        "llm_g = ChatOpenAI(model=MODEL, temperature=0)     # 가드/판정/리라이트\n",
        "llm_x = ChatOpenAI(model=MODEL, temperature=0.2)   # 자체 추출\n",
        "llm_s = ChatOpenAI(model=MODEL, temperature=0.2)   # 요약\n",
        "\n",
        "KNOWLEDGE_CUTOFF = \"2024-06\"\n",
        "\n",
        "STYLE_SYS = (\n",
        "    \"한국어 존댓말을 사용합니다. 말투는 친절하고 전문적으로 유지합니다. \"\n",
        "    \"핵심은 간결하게 전달하되, 전력·에너지·모빌리티 분야의 수치/단위/기호(η, THD, pf, pu, kW, kWh, °C 등)는 보존합니다. \"\n",
        "    \"불확실하거나 기억이 모호한 내용은 '불확실'로 표시하고 추정·일반론은 명확히 구분합니다. 과장 표현은 지양합니다.\"\n",
        ")\n",
        "\n",
        "ALLOW = [\n",
        "    \"전력\",\"에너지\",\"모빌리티\",\"EV\",\"충전\",\"배터리\",\"신재생\",\"태양광\",\"풍력\",\"수소\",\n",
        "    \"인버터\",\"컨버터\",\"PCS\",\"BESS\",\"V2G\",\"EMS\",\"DER\",\"DR\",\"스마트그리드\",\"송배전\",\"변전\",\n",
        "    \"THD\",\"pf\",\"pu\",\"고조파\",\"전압안정도\",\"신뢰도\",\"시장\",\"트렌드\",\"동향\"\n",
        "]\n",
        "\n",
        "def jload(s: str, default: Any) -> Any:\n",
        "    try:\n",
        "        return json.loads(s)\n",
        "    except:\n",
        "        return default\n",
        "\n",
        "def evaluate_answerability(q: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    JSON 예시\n",
        "    {\n",
        "      \"answerable\": true|false,\n",
        "      \"reasons\": [\"off_topic\",\"requires_web\",\"knowledge_cutoff\",\"ambiguous\",\"too_broad\",\"privacy_sensitive\",\"unsafe\"],\n",
        "      \"explain\": \"왜 바로 답하기 어려운지 한 줄 설명(존댓말)\",\n",
        "      \"ask_for\": [\"누락된 구체 정보 항목들\"],\n",
        "      \"rewrite_examples\": [\"권장 재질문 1\",\"권장 재질문 2\"]\n",
        "    }\n",
        "    \"\"\"\n",
        "    policy = (\n",
        "        \"당신은 질의가 아래 제약에서 답변 가능한지 판정합니다.\\n\"\n",
        "        f\"- 지식 컷오프: {KNOWLEDGE_CUTOFF}\\n\"\n",
        "        \"- 외부 웹 검색/실시간 데이터 사용 불가\\n\"\n",
        "        \"- 도메인: 전력·에너지·모빌리티 중심\\n\"\n",
        "        \"- 개인식별/민감정보, 불법/위험, 의료·법률·투자 개별 조언은 불가\\n\"\n",
        "        \"- 모호/과도하게 광범위/정의가 불명확하면 구체화가 필요\\n\"\n",
        "        \"위 조건을 바탕으로 JSON만 반환하세요.\"\n",
        "    )\n",
        "    schema = {\n",
        "        \"answerable\": True,\n",
        "        \"reasons\": [],\n",
        "        \"explain\": \"\",\n",
        "        \"ask_for\": [],\n",
        "        \"rewrite_examples\": []\n",
        "    }\n",
        "    prompt = (\n",
        "        policy + \"\\n\\n\"\n",
        "        \"가능하면 사용자가 바로 쓸 수 있는 '재질문 예시'를 1~3개 제안하세요 \"\n",
        "        \"(전력·에너지 맥락의 지표/범위/지역/기간/대상 등을 포함). \"\n",
        "        \"JSON만 출력. 스키마는 다음과 같음:\\n\"\n",
        "        + json.dumps(schema, ensure_ascii=False) + \"\\n\\n\"\n",
        "        f\"질문: {q}\"\n",
        "    )\n",
        "    out = llm_g.invoke([\n",
        "        SystemMessage(content=STYLE_SYS),\n",
        "        SystemMessage(content=\"JSON only.\"),\n",
        "        HumanMessage(content=prompt)\n",
        "    ]).content\n",
        "    data = jload(out, schema)\n",
        "    # 휴리스틱 보강: 실시간/최신/가격/주가/날씨 등\n",
        "    recent_triggers = [\"실시간\",\"현재\",\"오늘\",\"방금\",\"지금\",\"최신\",\"주가\",\"환율\",\"날씨\",\"스코어\",\"속보\",\"라이브\"]\n",
        "    if any(t in q for t in recent_triggers):\n",
        "        data[\"answerable\"] = False\n",
        "        if \"requires_web\" not in data[\"reasons\"]:\n",
        "            data[\"reasons\"].append(\"requires_web\")\n",
        "        data[\"explain\"] = data.get(\"explain\") or \"실시간·최신 데이터는 웹 접근 없이 정확히 안내드리기 어렵습니다.\"\n",
        "    # 도메인 체크\n",
        "    if not any(k.lower() in q.lower() for k in ALLOW):\n",
        "        if \"off_topic\" not in data[\"reasons\"]:\n",
        "            data[\"reasons\"].append(\"off_topic\")\n",
        "    return data\n",
        "\n",
        "def guidance_message(evald: Dict[str, Any]) -> str:\n",
        "    reasons_ko = {\n",
        "        \"off_topic\": \"준비된 주제 범위를 벗어났습니다.\",\n",
        "        \"requires_web\": \"실시간/웹 검색이 필요한 주제입니다.\",\n",
        "        \"knowledge_cutoff\": f\"지식 컷오프({KNOWLEDGE_CUTOFF}) 이후 정보가 필요합니다.\",\n",
        "        \"ambiguous\": \"질문이 모호합니다.\",\n",
        "        \"too_broad\": \"질문 범위가 너무 넓습니다.\",\n",
        "        \"privacy_sensitive\": \"개인정보·민감정보라서 도와드리기 어렵습니다.\",\n",
        "        \"unsafe\": \"위험/부적절한 요청입니다.\"\n",
        "    }\n",
        "    lines = [\"요청하신 내용은 바로 정확히 답변드리기 어렵습니다.\"]\n",
        "    if evald.get(\"explain\"):\n",
        "        lines.append(f\"사유: {evald['explain']}\")\n",
        "    if evald.get(\"reasons\"):\n",
        "        labs = [reasons_ko.get(r, r) for r in evald[\"reasons\"]]\n",
        "        lines.append(\"판정: \" + \", \".join(labs))\n",
        "    if evald.get(\"ask_for\"):\n",
        "        lines.append(\"\\n다음 정보를 알려주시면 더 정확히 도와드릴 수 있습니다:\")\n",
        "        lines += [f\"- {a}\" for a in evald[\"ask_for\"][:6]]\n",
        "    if evald.get(\"rewrite_examples\"):\n",
        "        lines.append(\"\\n이렇게 질문해 보시면 좋습니다:\")\n",
        "        lines += [f\"- {r}\" for r in evald[\"rewrite_examples\"][:3]]\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def make_subquestions(q: str) -> List[str]:\n",
        "    prompt = (\n",
        "        \"다음 질문을 3~5개의 하위 질문으로 분해하세요. 최신성 이슈가 있으면 '최근 동향 확인'을 포함합니다. \"\n",
        "        \"JSON만 출력: {\\\"subs\\\": [\\\"...\\\"]}\\n\\n\"\n",
        "        f\"질문: {q}\"\n",
        "    )\n",
        "    out = llm_x.invoke([SystemMessage(content=\"JSON only.\"), HumanMessage(content=prompt)]).content\n",
        "    data = jload(out, {\"subs\":[q]})\n",
        "    subs = [s for s in data.get(\"subs\", []) if isinstance(s, str) and s.strip()]\n",
        "    return subs[:5] if subs else [q]\n",
        "\n",
        "def extract_self(subq: str) -> Dict[str, Any]:\n",
        "    schema = {\n",
        "        \"bullets\": [],\n",
        "        \"metrics\": [{\"name\": \"\", \"value\": \"\", \"unit\": \"\", \"context\": \"\", \"confidence\": \"low|medium|high\"}],\n",
        "        \"claims\": [{\"statement\": \"\", \"confidence\": \"low|medium|high\"}],\n",
        "        \"uncertainties\": []\n",
        "    }\n",
        "    prompt = (\n",
        "        \"외부 검색 없이, 일반 지식 범위에서 아래 하위 질문의 핵심 정보를 자체적으로 수집·정리해 주세요. \"\n",
        "        \"확신도 표기, 불확실/근사 명시. JSON만 반환.\\n\\n\"\n",
        "        + json.dumps(schema, ensure_ascii=False)\n",
        "        + \"\\n\\n하위 질문:\\n\" + subq\n",
        "        + f\"\\n\\n지식 컷오프: {KNOWLEDGE_CUTOFF}\"\n",
        "    )\n",
        "    out = llm_x.invoke([\n",
        "        SystemMessage(content=STYLE_SYS),\n",
        "        SystemMessage(content=\"수치·단위·기호 보존, 불확실성 명시, JSON only.\"),\n",
        "        HumanMessage(content=prompt)\n",
        "    ]).content\n",
        "    return jload(out, {\"bullets\": [], \"metrics\": [], \"claims\": [], \"uncertainties\": []})\n",
        "\n",
        "def aggregate(parts: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    bullets, seen = [], set()\n",
        "    metrics, claims, uncertainties = [], [], []\n",
        "    for p in parts:\n",
        "        for b in p.get(\"bullets\", []):\n",
        "            s = b.strip()\n",
        "            if s and s not in seen:\n",
        "                seen.add(s); bullets.append(s)\n",
        "        metrics.extend([m for m in p.get(\"metrics\", []) if isinstance(m, dict)])\n",
        "        claims.extend([c for c in p.get(\"claims\", []) if isinstance(c, dict)])\n",
        "        uncertainties.extend([u for u in p.get(\"uncertainties\", []) if isinstance(u, str)])\n",
        "    return {\n",
        "        \"bullets\": bullets[:14],\n",
        "        \"metrics\": metrics[:12],\n",
        "        \"claims\": claims[:12],\n",
        "        \"uncertainties\": uncertainties[:8]\n",
        "    }\n",
        "\n",
        "def summarize(query: str, agg: Dict[str, Any]) -> str:\n",
        "    prompt = (\n",
        "        \"다음 정보를 바탕으로 카드형 요약을 작성해 주세요.\\n\"\n",
        "        \"- 섹션 3~6개, 각 3~5줄, 친절하고 전문적인 존댓말\\n\"\n",
        "        \"- 단위/수치/기호 보존, 추정/불확실 명시\\n\"\n",
        "        f\"- 지식 컷오프: {KNOWLEDGE_CUTOFF}\\n\\n\"\n",
        "        f\"질문: {query}\\n\"\n",
        "        f\"핵심포인트: {json.dumps(agg.get('bullets', []), ensure_ascii=False)}\\n\"\n",
        "        f\"지표: {json.dumps(agg.get('metrics', []), ensure_ascii=False)}\\n\"\n",
        "        f\"주장: {json.dumps(agg.get('claims', []), ensure_ascii=False)}\\n\"\n",
        "        f\"불확실: {json.dumps(agg.get('uncertainties', []), ensure_ascii=False)}\\n\"\n",
        "    )\n",
        "    return llm_s.invoke([\n",
        "        SystemMessage(content=STYLE_SYS),\n",
        "        SystemMessage(content=\"간결·정확·근거 수준 표시\"),\n",
        "        HumanMessage(content=prompt)\n",
        "    ]).content\n",
        "\n",
        "def run(query: str) -> str:\n",
        "    if not query.strip():\n",
        "        return \"질문을 입력해 주세요.\"\n",
        "    # ▶ NEW: 먼저 답변 가능성 평가\n",
        "    ev = evaluate_answerability(query)\n",
        "    if not ev.get(\"answerable\", True):\n",
        "        return guidance_message(ev)\n",
        "    # ▶ 가능하면 기존 파이프라인 수행\n",
        "    subs = make_subquestions(ev.get(\"rewrite_examples\",[query])[0] if ev.get(\"rewrite_examples\") else query)\n",
        "    parts = [extract_self(s) for s in subs]\n",
        "    agg = aggregate(parts)\n",
        "    return summarize(query, agg)\n",
        "\n",
        "def _chat_handler(user_text: str, history: list[list[str]]) -> str:\n",
        "    # 멀티턴 문맥을 쓰지 않고, 기존 단일턴 파이프라인(run) 그대로 호출\n",
        "    return run(user_text)\n",
        "\n",
        "# 그라디오 UI 색감 및 스타일 변경\n",
        "with gr.Blocks(theme=gr.themes.Soft(), title=\"전기·전자 요약 챗봇\") as demo:\n",
        "    gr.HTML(\n",
        "        \"\"\"\n",
        "        <div style=\"text-align: center; max-width: 700px; margin: 0 auto; padding: 20px;\">\n",
        "            <h1 style=\"color: #007bff; font-size: 2.5em; font-weight: bold; margin-bottom: 5px;\">🔌 전기·전자 요약 챗봇 💡</h1>\n",
        "            <p style=\"color: #555; font-size: 1.1em;\">시장/산업/업계 동향 관련 질문에 답해 드립니다.</p>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    gr.ChatInterface(\n",
        "        fn=_chat_handler,\n",
        "        chatbot=gr.Chatbot(\n",
        "            height=520,\n",
        "            show_copy_button=True,\n",
        "            render=False, # 상위 gr.Blocks에 직접 렌더링\n",
        "            layout=\"panel\"\n",
        "        ),\n",
        "        textbox=gr.Textbox(\n",
        "            placeholder=\"질문을 입력해 주세요. (예: 한국 재생에너지 보급 동향 요약)\",\n",
        "            container=False,\n",
        "            scale=7,\n",
        "        ),\n",
        "        examples=[\n",
        "            \"한국 재생에너지 보급 동향 요약\",\n",
        "            \"BESS 시장 트렌드 핵심 포인트\",\n",
        "            \"EV 충전 인프라 최근 이슈 정리\",\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    demo.launch(share=True) # Colab은 share=True 필요"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "KyYW53c5yao1",
        "outputId": "843fe7bf-37a6-4564-e7eb-288f143be1f1"
      },
      "id": "KyYW53c5yao1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3019489585.py:213: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot=gr.Chatbot(\n",
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:328: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://eef428fccb1c49a2ab.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://eef428fccb1c49a2ab.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "최종본(2)"
      ],
      "metadata": {
        "id": "_9xV0SFtpQhf"
      },
      "id": "_9xV0SFtpQhf"
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# 전기·전자 도메인 요약 챗봇 (주제 분류 기반 응답 집중형)\n",
        "\n",
        "# ▶ 사전 준비 (.env 파일 예시)\n",
        "# OPENAI_API_KEY=sk-xxxxx\n",
        "# OPENAI_MODEL=gpt-4o-mini\n",
        "\n",
        "import os\n",
        "import json\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "import gradio as gr\n",
        "\n",
        "# -----------------------------\n",
        "# 환경 변수 로드 (.env)\n",
        "# -----------------------------\n",
        "load_dotenv()\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    raise RuntimeError(\"OPENAI_API_KEY가 설정되어 있지 않습니다. .env 파일을 준비하거나 환경변수를 설정해 주세요.\")\n",
        "\n",
        "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
        "\n",
        "# -----------------------------\n",
        "# LLM 인스턴스\n",
        "# -----------------------------\n",
        "llm_g = ChatOpenAI(model=OPENAI_MODEL, temperature=0)       # 가드/판정/리라이트/분류\n",
        "llm_x = ChatOpenAI(model=OPENAI_MODEL, temperature=0.2)     # 자체 추출\n",
        "llm_s = ChatOpenAI(model=OPENAI_MODEL, temperature=0.2)     # 요약\n",
        "\n",
        "# -----------------------------\n",
        "# 정책/스타일\n",
        "# -----------------------------\n",
        "KNOWLEDGE_CUTOFF = \"2024-06\"\n",
        "\n",
        "STYLE_SYS = (\n",
        "    \"한국어 존댓말을 사용합니다. 말투는 친절하고 전문적으로 유지합니다. \"\n",
        "    \"핵심은 간결하게 전달하되, 전력·에너지·모빌리티 분야의 수치/단위/기호(η, THD, pf, pu, kW, kWh, °C 등)는 보존합니다. \"\n",
        "    \"불확실하거나 기억이 모호한 내용은 '불확실'로 표시하고 추정·일반론은 명확히 구분합니다. 과장 표현은 지양합니다.\"\n",
        ")\n",
        "\n",
        "# 허용 도메인(광범위하게 포함, 재생에너지 편향 제거)\n",
        "ALLOW = [\n",
        "    \"전력\",\"전력시장\",\"전기\",\"전기모터\",\"모터\",\"에너지\",\"모빌리티\",\"EV\",\"충전\",\"배터리\",\"BESS\",\n",
        "    \"신재생\",\"태양광\",\"풍력\",\"수소\",\"인버터\",\"컨버터\",\"PCS\",\"V2G\",\"EMS\",\"DER\",\"DR\",\n",
        "    \"스마트그리드\",\"송배전\",\"변전\",\"THD\",\"pf\",\"pu\",\"고조파\",\"전압안정도\",\"신뢰도\",\"시장\",\"트렌드\",\"동향\"\n",
        "]\n",
        "\n",
        "# 답변 주제 라벨\n",
        "TOPIC_LABELS = [\n",
        "    \"전력시장\", \"전기모터\", \"신재생에너지\", \"EV/충전\", \"배터리/BESS\",\n",
        "    \"스마트그리드\", \"송배전/변전\", \"기타\"\n",
        "]\n",
        "\n",
        "# -----------------------------\n",
        "# 유틸 함수\n",
        "# -----------------------------\n",
        "def jload(s: str, default: Any) -> Any:\n",
        "    try:\n",
        "        return json.loads(s)\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "# -----------------------------\n",
        "# 주제 분류기\n",
        "# -----------------------------\n",
        "def classify_topic(q: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    질문을 미리 정의한 토픽 중 하나로 분류.\n",
        "    출력 예:\n",
        "    {\n",
        "      \"topic\": \"전력시장\",  # TOPIC_LABELS 중 하나\n",
        "      \"subtopics\": [\"도매시장\", \"가격체계\"],\n",
        "      \"confidence\": \"high|medium|low\",\n",
        "      \"explain\": \"간단 근거\"\n",
        "    }\n",
        "    \"\"\"\n",
        "    schema = {\"topic\": \"\", \"subtopics\": [], \"confidence\": \"medium\", \"explain\": \"\"}\n",
        "    prompt = (\n",
        "        \"다음 질문을 아래 토픽 중 하나로만 분류하고 JSON으로만 답하세요.\\n\"\n",
        "        f\"가능한 토픽: {TOPIC_LABELS}\\n\"\n",
        "        \"규칙:\\n\"\n",
        "        \"- 질문의 핵심을 가장 잘 대표하는 단 하나의 topic을 고르세요.\\n\"\n",
        "        \"- subtopics는 1~3개, 없으면 빈 배열.\\n\"\n",
        "        \"- confidence는 high/medium/low 중 하나.\\n\"\n",
        "        \"- explain에는 간단한 근거를 1줄로.\\n\\n\"\n",
        "        f\"질문: {q}\\n\\n\"\n",
        "        f\"반환 스키마: {json.dumps(schema, ensure_ascii=False)}\"\n",
        "    )\n",
        "    out = llm_g.invoke([\n",
        "        SystemMessage(content=\"JSON only.\"),\n",
        "        HumanMessage(content=prompt)\n",
        "    ]).content\n",
        "    data = jload(out, schema)\n",
        "    if data.get(\"topic\") not in TOPIC_LABELS:\n",
        "        data[\"topic\"] = \"기타\"\n",
        "    if not isinstance(data.get(\"subtopics\"), list):\n",
        "        data[\"subtopics\"] = []\n",
        "    return data\n",
        "\n",
        "# -----------------------------\n",
        "# 파이프라인 단계 1: 답변 가능성 평가\n",
        "# -----------------------------\n",
        "def evaluate_answerability(q: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    JSON 예시\n",
        "    {\n",
        "      \"answerable\": true|false,\n",
        "      \"reasons\": [\"off_topic\",\"requires_web\",\"knowledge_cutoff\",\"ambiguous\",\"too_broad\",\"privacy_sensitive\",\"unsafe\"],\n",
        "      \"explain\": \"왜 바로 답하기 어려운지 한 줄 설명(존댓말)\",\n",
        "      \"ask_for\": [\"누락된 구체 정보 항목들\"],\n",
        "      \"rewrite_examples\": [\"권장 재질문 1\",\"권장 재질문 2\"]\n",
        "    }\n",
        "    \"\"\"\n",
        "    policy = (\n",
        "        \"당신은 질의가 아래 제약에서 답변 가능한지 판정합니다.\\n\"\n",
        "        f\"- 지식 컷오프: {KNOWLEDGE_CUTOFF}\\n\"\n",
        "        \"- 외부 웹 검색/실시간 데이터 사용 불가\\n\"\n",
        "        \"- 도메인: 전력·에너지·모빌리티 중심\\n\"\n",
        "        \"- 개인식별/민감정보, 불법/위험, 의료·법률·투자 개별 조언은 불가\\n\"\n",
        "        \"- 모호/과도하게 광범위/정의가 불명확하면 구체화가 필요\\n\"\n",
        "        \"위 조건을 바탕으로 JSON만 반환하세요.\"\n",
        "    )\n",
        "    schema = {\n",
        "        \"answerable\": True,\n",
        "        \"reasons\": [],\n",
        "        \"explain\": \"\",\n",
        "        \"ask_for\": [],\n",
        "        \"rewrite_examples\": []\n",
        "    }\n",
        "    prompt = (\n",
        "        policy + \"\\n\\n\"\n",
        "        \"가능하면 사용자가 바로 쓸 수 있는 '재질문 예시'를 1~3개 제안하세요 \"\n",
        "        \"(전력·에너지 맥락의 지표/범위/지역/기간/대상 등을 포함). \"\n",
        "        \"JSON만 출력. 스키마는 다음과 같음:\\n\"\n",
        "        + json.dumps(schema, ensure_ascii=False) + \"\\n\\n\"\n",
        "        f\"질문: {q}\"\n",
        "    )\n",
        "    out = llm_g.invoke([\n",
        "        SystemMessage(content=STYLE_SYS),\n",
        "        SystemMessage(content=\"JSON only.\"),\n",
        "        HumanMessage(content=prompt)\n",
        "    ]).content\n",
        "    data = jload(out, schema)\n",
        "\n",
        "    # 휴리스틱 보강: 실시간/최신/가격/주가/날씨 등\n",
        "    recent_triggers = [\"실시간\",\"현재\",\"오늘\",\"방금\",\"지금\",\"최신\",\"주가\",\"환율\",\"날씨\",\"스코어\",\"속보\",\"라이브\"]\n",
        "    if any(t in q for t in recent_triggers):\n",
        "        data[\"answerable\"] = False\n",
        "        if \"requires_web\" not in data[\"reasons\"]:\n",
        "            data[\"reasons\"].append(\"requires_web\")\n",
        "        data[\"explain\"] = data.get(\"explain\") or \"실시간·최신 데이터는 웹 접근 없이 정확히 안내드리기 어렵습니다.\"\n",
        "\n",
        "    # 도메인 체크(광범위 포함)\n",
        "    if not any(k.lower() in q.lower() for k in ALLOW):\n",
        "        if \"off_topic\" not in data[\"reasons\"]:\n",
        "            data[\"reasons\"].append(\"off_topic\")\n",
        "    return data\n",
        "\n",
        "# -----------------------------\n",
        "# 파이프라인 단계 1-보조: 가이드 문구 생성\n",
        "# -----------------------------\n",
        "def guidance_message(evald: Dict[str, Any]) -> str:\n",
        "    reasons_ko = {\n",
        "        \"off_topic\": \"준비된 주제 범위를 벗어났습니다.\",\n",
        "        \"requires_web\": \"실시간/웹 검색이 필요한 주제입니다.\",\n",
        "        \"knowledge_cutoff\": f\"지식 컷오프({KNOWLEDGE_CUTOFF}) 이후 정보가 필요합니다.\",\n",
        "        \"ambiguous\": \"질문이 모호합니다.\",\n",
        "        \"too_broad\": \"질문 범위가 너무 넓습니다.\",\n",
        "        \"privacy_sensitive\": \"개인정보·민감정보라서 도와드리기 어렵습니다.\",\n",
        "        \"unsafe\": \"위험/부적절한 요청입니다.\"\n",
        "    }\n",
        "    lines = [\"요청하신 내용은 바로 정확히 답변드리기 어렵습니다.\"]\n",
        "    if evald.get(\"explain\"):\n",
        "        lines.append(f\"사유: {evald['explain']}\")\n",
        "    if evald.get(\"reasons\"):\n",
        "        labs = [reasons_ko.get(r, r) for r in evald[\"reasons\"]]\n",
        "        lines.append(\"판정: \" + \", \".join(labs))\n",
        "    if evald.get(\"ask_for\"):\n",
        "        lines.append(\"\\n다음 정보를 알려주시면 더 정확히 도와드릴 수 있습니다:\")\n",
        "        lines += [f\"- {a}\" for a in evald[\"ask_for\"][:6]]\n",
        "    if evald.get(\"rewrite_examples\"):\n",
        "        lines.append(\"\\n이렇게 질문해 보시면 좋습니다:\")\n",
        "        lines += [f\"- {r}\" for r in evald[\"rewrite_examples\"][:3]]\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "# -----------------------------\n",
        "# 파이프라인 단계 2: 하위 질문 분해 (주제 고정)\n",
        "# -----------------------------\n",
        "def make_subquestions(q: str, topic: str) -> List[str]:\n",
        "    prompt = (\n",
        "        \"다음 질문을 3~5개의 하위 질문으로 분해하세요.\\n\"\n",
        "        f\"- 분류된 주제에만 집중하여 작성: {topic}\\n\"\n",
        "        \"- 최신성 이슈가 있으면 '최근 동향 확인'을 포함합니다.\\n\"\n",
        "        'JSON만 출력: {\"subs\": [\"...\"]}\\n\\n'\n",
        "        f\"질문: {q}\"\n",
        "    )\n",
        "    out = llm_x.invoke([SystemMessage(content=\"JSON only.\"), HumanMessage(content=prompt)]).content\n",
        "    data = jload(out, {\"subs\":[q]})\n",
        "    subs = [s for s in data.get(\"subs\", []) if isinstance(s, str) and s.strip()]\n",
        "    return subs[:5] if subs else [q]\n",
        "\n",
        "# -----------------------------\n",
        "# 파이프라인 단계 3: 자체 정보 추출 (주제 고정)\n",
        "# -----------------------------\n",
        "def extract_self(subq: str, topic: str, subtopics: List[str]) -> Dict[str, Any]:\n",
        "    schema = {\n",
        "        \"bullets\": [],\n",
        "        \"metrics\": [{\"name\": \"\", \"value\": \"\", \"unit\": \"\", \"context\": \"\", \"confidence\": \"low|medium|high\"}],\n",
        "        \"claims\": [{\"statement\": \"\", \"confidence\": \"low|medium|high\"}],\n",
        "        \"uncertainties\": []\n",
        "    }\n",
        "    prompt = (\n",
        "        \"외부 검색 없이, 일반 지식 범위에서 아래 하위 질문의 핵심 정보를 자체적으로 수집·정리해 주세요.\\n\"\n",
        "        f\"- 분류된 주제: {topic}\\n\"\n",
        "        f\"- 서브토픽(참고): {', '.join(subtopics) if subtopics else '없음'}\\n\"\n",
        "        \"- 분류된 주제와 무관한 분야(예: 다른 산업/기술)는 언급하지 마세요.\\n\"\n",
        "        \"- 확신도 표기, 불확실/근사 명시. JSON만 반환.\\n\\n\"\n",
        "        + json.dumps(schema, ensure_ascii=False)\n",
        "        + \"\\n\\n하위 질문:\\n\" + subq\n",
        "        + f\"\\n\\n지식 컷오프: {KNOWLEDGE_CUTOFF}\"\n",
        "    )\n",
        "    out = llm_x.invoke([\n",
        "        SystemMessage(content=STYLE_SYS),\n",
        "        SystemMessage(content=\"수치·단위·기호 보존, 불확실성 명시, JSON only.\"),\n",
        "        HumanMessage(content=prompt)\n",
        "    ]).content\n",
        "    return jload(out, {\"bullets\": [], \"metrics\": [], \"claims\": [], \"uncertainties\": []})\n",
        "\n",
        "# -----------------------------\n",
        "# 파이프라인 단계 4: 통합\n",
        "# -----------------------------\n",
        "def aggregate(parts: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    bullets, seen = [], set()\n",
        "    metrics, claims, uncertainties = [], [], []\n",
        "    for p in parts:\n",
        "        for b in p.get(\"bullets\", []):\n",
        "            s = b.strip()\n",
        "            if s and s not in seen:\n",
        "                seen.add(s); bullets.append(s)\n",
        "        metrics.extend([m for m in p.get(\"metrics\", []) if isinstance(m, dict)])\n",
        "        claims.extend([c for c in p.get(\"claims\", []) if isinstance(c, dict)])\n",
        "        uncertainties.extend([u for u in p.get(\"uncertainties\", []) if isinstance(u, str)])\n",
        "    return {\n",
        "        \"bullets\": bullets[:14],\n",
        "        \"metrics\": metrics[:12],\n",
        "        \"claims\": claims[:12],\n",
        "        \"uncertainties\": uncertainties[:8]\n",
        "    }\n",
        "\n",
        "# -----------------------------\n",
        "# 파이프라인 단계 5: 요약 생성 (주제 고정)\n",
        "# -----------------------------\n",
        "def summarize(query: str, agg: Dict[str, Any], topic: str, subtopics: List[str]) -> str:\n",
        "    topic_rule = (\n",
        "        f\"이번 응답은 반드시 '{topic}' 주제에만 집중해야 합니다. \"\n",
        "        \"분류된 주제와 직접적 관련이 없는 타 분야(예: 다른 에너지 기술/산업)는 언급하지 않습니다.\"\n",
        "    )\n",
        "    prompt = (\n",
        "        \"다음 정보를 바탕으로 카드형 요약을 작성해 주세요.\\n\"\n",
        "        \"- 섹션 3~6개, 각 3~5줄, 친절하고 전문적인 존댓말\\n\"\n",
        "        \"- 단위/수치/기호 보존, 추정/불확실 명시\\n\"\n",
        "        f\"- 지식 컷오프: {KNOWLEDGE_CUTOFF}\\n\"\n",
        "        f\"- 주제 규칙: {topic_rule}\\n\"\n",
        "        f\"- 서브토픽(참고): {', '.join(subtopics) if subtopics else '없음'}\\n\\n\"\n",
        "        f\"질문: {query}\\n\"\n",
        "        f\"핵심포인트: {json.dumps(agg.get('bullets', []), ensure_ascii=False)}\\n\"\n",
        "        f\"지표: {json.dumps(agg.get('metrics', []), ensure_ascii=False)}\\n\"\n",
        "        f\"주장: {json.dumps(agg.get('claims', []), ensure_ascii=False)}\\n\"\n",
        "        f\"불확실: {json.dumps(agg.get('uncertainties', []), ensure_ascii=False)}\\n\"\n",
        "    )\n",
        "    return llm_s.invoke([\n",
        "        SystemMessage(content=STYLE_SYS),\n",
        "        SystemMessage(content=\"간결·정확·근거 수준 표시\"),\n",
        "        HumanMessage(content=prompt)\n",
        "    ]).content\n",
        "\n",
        "# -----------------------------\n",
        "# 실행 핸들러\n",
        "# -----------------------------\n",
        "def run(query: str) -> str:\n",
        "    if not query.strip():\n",
        "        return \"질문을 입력해 주세요.\"\n",
        "    # 0) 주제 분류\n",
        "    cls = classify_topic(query)\n",
        "    topic = cls.get(\"topic\", \"기타\")\n",
        "    subtopics = cls.get(\"subtopics\", [])\n",
        "    # 1) 답변 가능성 평가\n",
        "    ev = evaluate_answerability(query)\n",
        "    if not ev.get(\"answerable\", True):\n",
        "        # 가이드 메시지에 주제 정보 보강\n",
        "        guide = guidance_message(ev)\n",
        "        guide += f\"\\n\\n[분류 결과] 주제: {topic}, 신뢰도: {cls.get('confidence','medium')}\"\n",
        "        return guide\n",
        "    # 2) 하위 질문 분해 (주제 고정)\n",
        "    base_q = ev.get(\"rewrite_examples\", [query])[0] if ev.get(\"rewrite_examples\") else query\n",
        "    subs = make_subquestions(base_q, topic)\n",
        "    # 3) 자체 추출\n",
        "    parts = [extract_self(s, topic, subtopics) for s in subs]\n",
        "    # 4) 통합\n",
        "    agg = aggregate(parts)\n",
        "    # 5) 요약 생성 (주제 고정)\n",
        "    return summarize(query, agg, topic, subtopics)\n",
        "\n",
        "def _chat_handler(user_text: str, history: list[list[str]]) -> str:\n",
        "    # 멀티턴 문맥을 쓰지 않고, 기존 단일턴 파이프라인(run) 그대로 호출\n",
        "    return run(user_text)\n",
        "\n",
        "# -----------------------------\n",
        "# Gradio UI\n",
        "# -----------------------------\n",
        "with gr.Blocks(theme=gr.themes.Soft(), title=\"전기·전자 요약 챗봇(주제 분류형)\") as demo:\n",
        "    gr.HTML(\n",
        "        \"\"\"\n",
        "        <div style=\"text-align: center; max-width: 700px; margin: 0 auto; padding: 20px;\">\n",
        "            <h1 style=\"color: #007bff; font-size: 2.2em; font-weight: bold; margin-bottom: 5px;\">🔌 전기·전자 요약 챗봇 💡</h1>\n",
        "            <p style=\"color: #555; font-size: 1.02em;\">질문을 자동 분류(전력시장/전기모터/신재생/EV·충전/배터리·BESS/스마트그리드/송배전·변전)하여 주제에 맞게 요약합니다.</p>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    gr.ChatInterface(\n",
        "        fn=_chat_handler,\n",
        "        chatbot=gr.Chatbot(\n",
        "            height=520,\n",
        "            show_copy_button=True,\n",
        "            render=False, # 상위 gr.Blocks에 직접 렌더링\n",
        "            layout=\"panel\"\n",
        "        ),\n",
        "        textbox=gr.Textbox(\n",
        "            placeholder=\"질문을 입력해 주세요. (예: 한국 도매 전력시장 가격 결정 구조 요약)\",\n",
        "            container=False,\n",
        "            scale=7,\n",
        "        ),\n",
        "        examples=[\n",
        "            \"한국 도매 전력시장 가격 결정 구조 요약\",\n",
        "            \"영구자석 동기모터(PMSM) 고효율 설계 핵심\",\n",
        "            \"태양광 LCOE 구성요소와 민감도\",\n",
        "            \"DC 급속충전 인프라 과제\",\n",
        "            \"BESS 사이클 수명과 열관리 이슈\",\n",
        "            \"분산자원 연계형 스마트그리드 요소\",\n",
        "            \"송배전망에서 전압안정도 지표 정리\",\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    demo.launch(share=True)  # Colab은 share=True 필요\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "nI5oMSKunYTW",
        "outputId": "0fface0d-4510-4de8-bc99-d984c44f1ec7"
      },
      "id": "nI5oMSKunYTW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3716230287.py:326: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot=gr.Chatbot(\n",
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:328: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://30f94fa28a429f7662.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://30f94fa28a429f7662.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}